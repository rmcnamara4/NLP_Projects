## ðŸ§  Applied NLP & LLM Engineering Portfolio

A 10-week learning journey of progressively complex NLP projects, evolving from classical methods to transformer-based pipelines, with modular, production-ready code for real-world ML workflows.

---

### ðŸš€ Highlights

- **End-to-end NLP workflows**: preprocessing â†’ embeddings â†’ RNNs â†’ transformers â†’ LLM applications.  
- **Applied, engineering-oriented approach** with reusable modules for data ingestion, cleaning, tokenization, training, and evaluation.  
- **Models implemented from scratch** (Transformer architecture, attention mechanism) and with modern frameworks (Hugging Face, PyTorch).  
- **Production-minded structure**: argument parsing, logging, and organized pipelines for reproducibility.  
- **Code quality progression** visible across weeks, showing growth in modular scripts, configs, and logging for reproducibility.

---

### ðŸ“‚ Repo Structure

Each folder contains notebooks, scripts, and notes for a thematic week:

1. Core NLP foundations  
2. Classical NLP techniques  
3. Word embeddings & distributional semantics  
4. Deep learning for NLP (RNNs, LSTMs)  
5. Transformer architecture from scratch  
6. Encoder-centric models (BERT, RoBERTa)  
7. Decoder-centric models (GPT-style)  
8. Other transformer architectures  
9. LLM scaling & prompt engineering  
10. Retrieval-Augmented Generation (RAG) & knowledge integration  

---

This repo serves as **both a learning record and an engineering portfolio** â€” demonstrating practical NLP expertise, sound ML engineering practices, and the ability to build real-world pipelines end-to-end.