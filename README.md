## ðŸ§  NLP Learning Journey â€” From Fundamentals to LLMs

A **10-week structured progression** through NLP and LLMs, moving from classical techniques to modern transformer-based workflows.  
Over the course of the project, both the **complexity of models** and the **engineering rigor** increased â€” with later weeks reflecting more modular, reproducible, and production-ready code.

---

### ðŸš€ Highlights

- **End-to-end NLP workflows**: preprocessing â†’ embeddings â†’ RNNs â†’ transformers â†’ LLM applications.  
- **Applied, engineering-oriented approach** with reusable modules for data ingestion, cleaning, tokenization, training, and evaluation.  
- **Models implemented from scratch** (Transformer architecture, attention mechanism) and with modern frameworks (Hugging Face, PyTorch).  
- **Production-minded structure**: argument parsing, logging, and organized pipelines for reproducibility.  
- **Code quality progression** visible across weeks, showing growth in modular scripts, configs, and logging for reproducibility.

---

### ðŸ“‚ Repo Structure

Each folder contains notebooks, scripts, and notes for a thematic week:

1. Core NLP foundations  
2. Classical NLP techniques  
3. Word embeddings & distributional semantics  
4. Deep learning for NLP (RNNs, LSTMs)  
5. Transformer architecture from scratch  
6. Encoder-centric models (BERT, RoBERTa)  
7. Decoder-centric models (GPT-style)  
8. Other transformer architectures  
9. LLM scaling & prompt engineering  
10. Retrieval-Augmented Generation (RAG) & knowledge integration  

---

This repo serves as **both a learning record and an engineering portfolio** â€” demonstrating practical NLP expertise, sound ML engineering practices, and the ability to build real-world pipelines end-to-end.