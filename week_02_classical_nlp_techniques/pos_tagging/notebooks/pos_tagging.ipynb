{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hidden Markov Model for Part-of-Speech Tagging\n",
        "\n",
        "This notebook implements a Hidden Markov Model (HMM) from scratch to perform part-of-speech (POS) tagging using the Brown corpus with the universal tagset. POS tagging is a classic sequence labeling task in NLP where each word in a sentence is assigned its grammatical category (e.g., noun, verb, adjective).\n",
        "\n",
        "We walk through the following steps:\n",
        "\n",
        "- **Data Preparation**: Load and split the Brown corpus into training and test sets.\n",
        "- **Frequency Counts**: Compute transition and emission counts needed for estimating probabilities.\n",
        "- **Probability Estimation**: Calculate initial, transition, and emission probabilities using maximum likelihood estimates.\n",
        "- **Viterbi Algorithm**: Implement the Viterbi algorithm to infer the most likely sequence of POS tags for a sentence.\n",
        "- **Evaluation**: Assess tagging accuracy on the test set.\n",
        "\n",
        "This notebook offers a hands-on understanding of how probabilistic sequence models like HMMs work under the hood, before diving into neural sequence models. This is part of **Week 2 - Classical NLP Techniques** in my learning journey."
      ],
      "metadata": {
        "id": "pVimasHJ7fLZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "_4L50EwQ7jsw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qxnrmexG7d8W"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from collections import defaultdict\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download necessary nltk packages that we will use as our dataset\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')"
      ],
      "metadata": {
        "id": "HjO5PsE27y_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e692dca-4247-4e5c-ce3d-c5eeb0306ea0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Splitting the Data\n",
        "\n",
        "We begin by loading the [Brown Corpus](https://www.nltk.org/nltk_data/) using the `universal` tagset, which provides a simplified set of part-of-speech (POS) tags. This tagset groups POS labels into categories like `NOUN`, `VERB`, `ADJ`, etc., making it easier to interpret results.\n",
        "\n",
        "The dataset is then split into training and testing sets using an 80/20 split. The training data will be used to estimate the probabilities required by the Hidden Markov Model (HMM), while the test data will be used to evaluate the model’s tagging performance."
      ],
      "metadata": {
        "id": "3h1ZB6Ku8Uka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_sentences = brown.tagged_sents(tagset = 'universal')"
      ],
      "metadata": {
        "id": "kF5I59RY8VEk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(len(tagged_sentences) * 0.8)\n",
        "train_sentences = tagged_sentences[:train_size]\n",
        "test_sentences = tagged_sentences[train_size:]"
      ],
      "metadata": {
        "id": "1GCU9rga8zgM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Length of train sentences:', len(train_sentences))\n",
        "print('Length of test senetences:', len(test_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6_BVNrH9OHi",
        "outputId": "fd786aeb-1ce1-4b42-c838-1586d45d6e2f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train sentences: 45872\n",
            "Length of test senetences: 11468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vocabulary and Tag Distribution\n",
        "\n",
        "Next, we extract the words and tags from the training sentences:\n",
        "\n",
        "- **Words** are lowercased to reduce sparsity in the vocabulary.\n",
        "- **Tags** are collected from the part-of-speech annotations.\n",
        "\n",
        "We then compute:\n",
        "- The set of unique words and tags,\n",
        "- The frequency of each word and tag using Python’s `Counter`.\n",
        "\n",
        "This gives us insight into the size of our vocabulary and the distribution of POS tags, which will be useful for initializing and smoothing our Hidden Markov Model. We also print the 10 most common words and a full distribution of tag frequencies."
      ],
      "metadata": {
        "id": "mkh5tYAV9pUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [word.lower() for sent in train_sentences for word, _ in sent]\n",
        "tags = [tag for sent in train_sentences for _, tag in sent]"
      ],
      "metadata": {
        "id": "nKJkN7iw9Tb_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_words = set(words)\n",
        "unique_tags = set(tags)"
      ],
      "metadata": {
        "id": "fyXra_iT-Yzl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts = Counter(words)\n",
        "tag_counts = Counter(tags)"
      ],
      "metadata": {
        "id": "6EhYJbVX-e3l"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of unique words:', len(unique_words))\n",
        "print('Number of unique tags:', len(unique_tags))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw2s-wQc9U2E",
        "outputId": "4b3af4af-c08c-4c18-dbe9-e51091a1a1a3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words: 45755\n",
            "Number of unique tags: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Most common words:', word_counts.most_common(10))\n",
        "print('Most common tags:', tag_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsfAx_HC-d0F",
        "outputId": "07492488-ca45-43c8-8420-370914a0cb89"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common words: [('the', 61191), (',', 48492), ('.', 39534), ('of', 32942), ('and', 24281), ('to', 22423), ('a', 19499), ('in', 18935), ('is', 9671), ('that', 9009)]\n",
            "Most common tags: Counter({'NOUN': 241528, 'VERB': 150459, 'ADP': 126332, '.': 118482, 'DET': 116989, 'ADJ': 73866, 'ADV': 45940, 'PRON': 35550, 'CONJ': 32177, 'PRT': 23316, 'NUM': 13802, 'X': 1205})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constructing HMM Count Matrices\n",
        "\n",
        "We now build the count matrices required for a Hidden Markov Model (HMM):\n",
        "\n",
        "- **Emission Counts**: How often each word is observed given a tag (P(word | tag)).\n",
        "- **Transition Counts**: How often each tag follows another tag (P(tag_t | tag_{t-1})).\n",
        "- **Initial Tag Counts**: Frequency of tags that appear at the beginning of a sentence.\n",
        "\n",
        "This loop goes through each sentence in the training data and:\n",
        "- Increments the emission count for the observed `(tag, word)` pair.\n",
        "- Tracks initial tag counts for the first tag in every sentence.\n",
        "- Updates transition counts between consecutive tags.\n",
        "\n",
        "These frequency counts will later be normalized to form probability matrices for decoding with the Viterbi algorithm."
      ],
      "metadata": {
        "id": "HEuA1uUc-9z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transition_counts = defaultdict(lambda: defaultdict(int))\n",
        "emission_counts = defaultdict(lambda: defaultdict(int))\n",
        "tag_counts = dict(tag_counts)\n",
        "initial_counts = defaultdict(int)"
      ],
      "metadata": {
        "id": "GrRN40Oh-l15"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in train_sentences:\n",
        "  prev_tag = None\n",
        "  for i, (word, tag) in enumerate(sentence):\n",
        "    word = word.lower()\n",
        "    emission_counts[tag][word] += 1\n",
        "\n",
        "    if i == 0:\n",
        "      initial_counts[tag] += 1\n",
        "    else:\n",
        "      transition_counts[prev_tag][tag] += 1\n",
        "\n",
        "    prev_tag = tag"
      ],
      "metadata": {
        "id": "wGi1Kddk_MYX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting Counts to Probabilities\n",
        "\n",
        "Once we’ve collected the raw frequency counts for transitions, emissions, and initial tags, we convert them into probabilities.\n",
        "\n",
        "- **Transition Probabilities**: Normalize each row of the `transition_counts` dictionary so that each value represents P(tag_t | tag_{t−1}).\n",
        "- **Emission Probabilities**: Normalize each row of the `emission_counts` dictionary to represent P(word | tag).\n",
        "- **Initial Probabilities**: Normalize the tag frequencies that appear at the start of each sentence to get P(tag_0).\n",
        "\n",
        "This step turns the HMM components into valid probability distributions, which we’ll use to perform POS tagging on unseen sentences."
      ],
      "metadata": {
        "id": "NIdKqjz1hOec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_counts_to_probabilities(counts):\n",
        "  \"\"\"\n",
        "  Convert nested count dictionaries to probability distributions.\n",
        "\n",
        "  Parameters:\n",
        "      counts (dict): A nested dictionary where each outer key maps to a dictionary\n",
        "                      of counts (e.g., {state: {next_state: count, ...}}).\n",
        "\n",
        "  Returns:\n",
        "      dict: A nested dictionary with the same structure, but with normalized\n",
        "            probabilities instead of raw counts.\n",
        "            (e.g., {state: {next_state: prob, ...}})\n",
        "  \"\"\"\n",
        "  probs = {}\n",
        "  for key, counts in counts.items():\n",
        "    total = sum(counts.values())\n",
        "    probs[key] = {k: v / total for k, v in counts.items()}\n",
        "  return probs"
      ],
      "metadata": {
        "id": "tYvvzAMjAeh0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transition_probabilities = convert_counts_to_probabilities(transition_counts)\n",
        "emission_probabilities = convert_counts_to_probabilities(emission_counts)\n",
        "initial_probabilities = {k: v / sum(initial_counts.values()) for k, v in initial_counts.items()}"
      ],
      "metadata": {
        "id": "sh3kpMm9BbFn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Emission Probability Function\n",
        "\n",
        "This function retrieves the probability of a word given a specific tag. If the word was not seen with the tag during training, it returns a small smoothing value to prevent zero probability."
      ],
      "metadata": {
        "id": "_RadBLtRB8UE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_emission_probability(emission_probs, tag, word, smoothing = 1e-6):\n",
        "  \"\"\"\n",
        "  Retrieve the emission probability of a word given a tag, with optional smoothing.\n",
        "\n",
        "  Args:\n",
        "      emission_probs (dict): A nested dictionary of emission probabilities,\n",
        "                              where emission_probs[tag][word] gives P(word | tag).\n",
        "      tag (str): The POS tag or state.\n",
        "      word (str): The observed word/token.\n",
        "      smoothing (float, optional): A small fallback probability used if the word\n",
        "                                    is not found under the given tag. Default is 1e-6.\n",
        "\n",
        "  Returns:\n",
        "      float: The emission probability P(word | tag), or the smoothing value if the word is unseen.\n",
        "\n",
        "  Notes:\n",
        "      - This function uses add-ε smoothing to handle unseen word-tag pairs.\n",
        "      - If the word is not found in the emission dictionary for the given tag,\n",
        "        the function returns the smoothing value instead.\n",
        "  \"\"\"\n",
        "  return emission_probs[tag].get(word, smoothing)"
      ],
      "metadata": {
        "id": "abxtqA0jBxyN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Viterbi Algorithm for POS Tagging\n",
        "\n",
        "This function implements the Viterbi algorithm to find the most probable sequence of tags for a given sentence.\n",
        "\n",
        "It uses:\n",
        "- **Initial probabilities** for the first word.\n",
        "- **Transition probabilities** between tags.\n",
        "- **Emission probabilities** of a word given a tag.\n",
        "\n",
        "The algorithm builds a dynamic programming table (`v_table`) and a backpointer matrix to trace the best tag sequence. The output is a list of (word, tag) pairs for the sentence along with the overall path probability."
      ],
      "metadata": {
        "id": "gPCqx8COhjX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def viterbi(sentence, initial_probs, transition_probs, emission_probs, unique_tags):\n",
        "  \"\"\"\n",
        "  Apply the Viterbi algorithm to find the most likely sequence of tags for a given sentence.\n",
        "\n",
        "  Parameters:\n",
        "      sentence (list of str): A list of words representing the observed sequence.\n",
        "      initial_probs (dict): A dictionary of initial probabilities for each tag (P(tag_0)).\n",
        "      transition_probs (dict): A nested dictionary representing transition probabilities\n",
        "                                between tags (P(tag_t | tag_{t-1})).\n",
        "      emission_probs (function): A function that returns the emission probability\n",
        "                                  P(word | tag), typically with smoothing support.\n",
        "      unique_tags (set or list of str): The set of all possible tags used in the model.\n",
        "\n",
        "  Returns:\n",
        "      tuple:\n",
        "          - list of (word, tag) pairs representing the most likely tag sequence.\n",
        "          - float: the probability of the best tag sequence.\n",
        "\n",
        "  Notes:\n",
        "      - Uses log probabilities to avoid numerical underflow during multiplication.\n",
        "      - Applies add-one smoothing (1e-6) for unseen emission and transition probabilities.\n",
        "      - Backpointers are used to reconstruct the best tag sequence from the dynamic programming table.\n",
        "  \"\"\"\n",
        "  words = [word.lower() for word in sentence]\n",
        "  n = len(sentence)\n",
        "  m = len(unique_tags)\n",
        "  tags_list = list(unique_tags)\n",
        "\n",
        "  v_table = np.zeros((m, n))\n",
        "  backpointer = np.zeros((m, n), dtype = int)\n",
        "\n",
        "  for i, tag in enumerate(tags_list):\n",
        "    v_table[i, 0] = math.log(initial_probs.get(tag, 1e-6)) + math.log(get_emission_probability(emission_probs, tag, words[0]))\n",
        "\n",
        "  for i in range(1, n):\n",
        "    for j in range(m):\n",
        "      temp_values = []\n",
        "      for k in range(m):\n",
        "        temp_values.append(\n",
        "            v_table[k, i - 1] + math.log(transition_probs.get(tags_list[k], {}).get(tags_list[j], 1e-6)) + math.log(get_emission_probability(emission_probs, tags_list[j], words[i]))\n",
        "        )\n",
        "      v_table[j, i] = max(temp_values)\n",
        "      backpointer[j, i] = np.argmax(temp_values)\n",
        "\n",
        "  best_path_prob = max(v_table[:, -1])\n",
        "  best_path_pointer = np.argmax(v_table[:, -1])\n",
        "\n",
        "  best_path = [best_path_pointer]\n",
        "  for i in range(n - 1, 0, -1):\n",
        "    best_path.append(int(backpointer[best_path[-1], i]))\n",
        "\n",
        "  best_tags = [tags_list[i] for i in best_path[::-1]]\n",
        "\n",
        "  return list(zip(sentence, best_tags)), math.exp(best_path_prob)"
      ],
      "metadata": {
        "id": "a13Yg7Q2CKEX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the POS Tagger\n",
        "\n",
        "This function evaluates the overall tagging accuracy of the Viterbi decoder on the test set. It compares predicted tags to ground truth tags and reports the proportion of correct predictions."
      ],
      "metadata": {
        "id": "zqgluf5qhyxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(test_data, initial_probs, transition_probs, emission_probs, unique_tags):\n",
        "  \"\"\"\n",
        "  Evaluate the accuracy of a sequence tagging model using the Viterbi algorithm.\n",
        "\n",
        "  Parameters:\n",
        "      test_data (list of list of (str, str)): A list of sentences, where each sentence is a\n",
        "                                              list of (word, true_tag) pairs.\n",
        "      initial_probs (dict): Initial tag probabilities P(tag_0).\n",
        "      transition_probs (dict): Transition probabilities P(tag_t | tag_{t-1}).\n",
        "      emission_probs (function): A function that returns emission probabilities P(word | tag).\n",
        "      unique_tags (set or list): The full set of possible tags.\n",
        "\n",
        "  Returns:\n",
        "      float: The overall tagging accuracy (correct predictions / total tags).\n",
        "\n",
        "  Notes:\n",
        "      - Uses the Viterbi decoder to generate predicted tag sequences.\n",
        "      - Assumes test data uses the same tag set and vocabulary as the training data.\n",
        "  \"\"\"\n",
        "  correct, total = 0, 0\n",
        "  for sentence in test_data:\n",
        "    words, true_tags = zip(*sentence)\n",
        "\n",
        "    predictions, probability = viterbi(words, initial_probs, transition_probs, emission_probs, unique_tags)\n",
        "    predicted_tags = [tag for _, tag in predictions]\n",
        "\n",
        "    correct += sum(pred == true for pred, true in zip(predicted_tags, true_tags))\n",
        "    total += len(true_tags)\n",
        "\n",
        "  return correct / total"
      ],
      "metadata": {
        "id": "mIEnmqYcMDke"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "After running the Viterbi algorithm on the test set, we compute the overall tagging accuracy. This reflects how well our HMM-based POS tagger generalizes to unseen data."
      ],
      "metadata": {
        "id": "3vArsBmDNEaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluate(test_sentences, initial_probabilities, transition_probabilities, emission_probabilities, unique_tags)\n",
        "print(f'Accuracy: {accuracy*100:.4f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvltHinvMjJA",
        "outputId": "37040ced-225a-4598-8159-c1f99e04d31d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 94.6014%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a0A4CwZEYvQY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}