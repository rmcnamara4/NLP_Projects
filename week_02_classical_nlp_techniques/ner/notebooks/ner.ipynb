{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_RO8FqLSlRv"
      },
      "source": [
        "# Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b4_l9AU9Ta-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25011d3e-8f2e-4e8d-abb2-a4e79a8a327c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn-crfsuite\n",
            "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting spacy-transformers\n",
            "  Downloading spacy_transformers-1.3.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting python-crfsuite>=0.9.7 (from sklearn-crfsuite)\n",
            "  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite) (1.6.1)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (2.0.2)\n",
            "Requirement already satisfied: spacy<4.1.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (3.8.5)\n",
            "Collecting transformers<4.50.0,>=3.4.0 (from spacy-transformers)\n",
            "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (2.5.1)\n",
            "Collecting spacy-alignments<1.0.0,>=0.7.2 (from spacy-transformers)\n",
            "  Downloading spacy_alignments-0.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite) (3.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.1.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (0.15.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->spacy-transformers) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (0.30.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (0.5.3)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.1.2)\n",
            "Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading spacy_transformers-1.3.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (756 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.2/756.2 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy_alignments-0.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.0/314.0 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=74262eeb6783b4e214febd58896c1a99a115eb43cd24d40de9c1d1e40b93d237\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
            "Successfully built seqeval\n",
            "Installing collected packages: spacy-alignments, python-crfsuite, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, sklearn-crfsuite, seqeval, nvidia-cusolver-cu12, transformers, spacy-transformers\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 python-crfsuite-0.9.11 seqeval-1.2.2 sklearn-crfsuite-0.5.0 spacy-alignments-0.9.1 spacy-transformers-1.3.8 transformers-4.49.0\n"
          ]
        }
      ],
      "source": [
        "# install required libraries\n",
        "!pip install sklearn-crfsuite seqeval spacy-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4voUTSwYELx",
        "outputId": "cd80c626-544b-4404-94d8-c65cf0839e4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-trf==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.8.0/en_core_web_trf-3.8.0-py3-none-any.whl (457.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.4/457.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy-curated-transformers<1.0.0,>=0.2.2 (from en-core-web-trf==3.8.0)\n",
            "  Downloading spacy_curated_transformers-0.3.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting curated-transformers<0.2.0,>=0.1.0 (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0)\n",
            "  Downloading curated_transformers-0.1.1-py2.py3-none-any.whl.metadata (965 bytes)\n",
            "Collecting curated-tokenizers<0.1.0,>=0.0.9 (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0)\n",
            "  Downloading curated_tokenizers-0.0.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: regex>=2022 in /usr/local/lib/python3.11/dist-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.0.2)\n",
            "Downloading spacy_curated_transformers-0.3.0-py2.py3-none-any.whl (236 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.3/236.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading curated_tokenizers-0.0.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (735 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m735.6/735.6 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading curated_transformers-0.1.1-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: curated-tokenizers, curated-transformers, spacy-curated-transformers, en-core-web-trf\n",
            "Successfully installed curated-tokenizers-0.0.9 curated-transformers-0.1.1 en-core-web-trf-3.8.0 spacy-curated-transformers-0.3.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_trf')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# download spacy model\n",
        "!python -m spacy download en_core_web_trf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K9ugs7xVe9A",
        "outputId": "1e0e9b58-fff7-4384-d951-07df81130b0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "project_path = '/content/drive/MyDrive/NLP_Projects/Week_2/ner'\n",
        "os.chdir(project_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZHZYRi6Soet"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OR1iLafSSZV9"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus.reader import ConllCorpusReader\n",
        "\n",
        "from sklearn_crfsuite import CRF, scorers, metrics\n",
        "from sklearn_crfsuite.metrics import flat_f1_score\n",
        "\n",
        "import time\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from seqeval.metrics import classification_report, f1_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from tqdm import tqdm\n",
        "# from spacy.util import use_gpu\n",
        "\n",
        "import re\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, cross_val_score, PredefinedSplit\n",
        "from sklearn.metrics import make_scorer\n",
        "from scipy.stats import loguniform"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if GPU is available for SpaCy model\n",
        "print(\"spaCy GPU Available:\", spacy.prefer_gpu())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hacrtZqNHhaM",
        "outputId": "e315d5fb-1d14-4549-b3f0-2fc7bacefaa8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy GPU Available: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load SpaCy's transformer-based English model (`en_core_web_trf`) which leverages pretrained transformer embeddings (like BERT) for more accurate natural language processing tasks such as NER and POS tagging."
      ],
      "metadata": {
        "id": "6jvoQAPNI_hl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t1VDLx11XiIu"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_trf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWrX8ndXUS3F"
      },
      "source": [
        "## Explore the CoNLL-2003 Training and Test Data\n",
        "\n",
        "We load the CoNLL-2003 dataset using NLTK’s `ConllCorpusReader` and extract BIO-tagged sentences for named entity recognition. This section provides basic statistics about the dataset:\n",
        "\n",
        "- **Training and Test Size**: Number of sentences and total token-level observations in both the training and test splits.\n",
        "- **Vocabulary and Label Space**: Counts of unique words and entity types present in the training data.\n",
        "- **Distribution Summary**: The 10 most frequent words and entities to help us understand class imbalance or dominant terms in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6BkYcm0MTUXE"
      },
      "outputs": [],
      "source": [
        "train = ConllCorpusReader('./Data/', 'eng.train', ['words', 'pos', 'ignore', 'chunk'])\n",
        "test_a = ConllCorpusReader('./Data', 'eng.testa', ['words', 'pos', 'ignore', 'chunk'])\n",
        "test_b = ConllCorpusReader('./Data', 'eng.testb', ['words', 'pos', 'ignore', 'chunk'])\n",
        "\n",
        "train_sentences = train.iob_sents()\n",
        "test_sentences = test_a.iob_sents() + test_b.iob_sents()\n",
        "\n",
        "train_sentences = [sent for sent in train_sentences if len(sent) > 0]\n",
        "test_sentences = [sent for sent in test_sentences if len(sent) > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RB13oZYsUa6i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f851423-3229-4e27-85cc-5b44d372fae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences in train: 14041\n",
            "Number of sentences in test: 6703\n",
            "\n",
            "Total number of observations in train: 203621\n",
            "Total number of observations in test: 97797\n"
          ]
        }
      ],
      "source": [
        "print('Number of sentences in train:', len(train_sentences))\n",
        "print('Number of sentences in test:', len(test_sentences))\n",
        "print()\n",
        "print('Total number of observations in train:', sum(map(len, train_sentences)))\n",
        "print('Total number of observations in test:', sum(map(len, test_sentences)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_o2IfhTzaGrH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "100f7659-5f59-4f11-a8c3-384e225e64e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words: 23623\n",
            "Number of unique entities: 9\n"
          ]
        }
      ],
      "source": [
        "words = [word for sentence in train_sentences for word, _, _ in sentence]\n",
        "entities = [ent for sentence in train_sentences for _, _, ent in sentence]\n",
        "\n",
        "print('Number of unique words:', len(set(words)))\n",
        "print('Number of unique entities:', len(set(entities)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "arbOv6Q3aVEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1a1c285-5860-44a5-e806-b7bea0542528"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ten most common words: [('.', 7374), (',', 7290), ('the', 7243), ('of', 3751), ('in', 3398), ('to', 3382), ('a', 2994), ('(', 2861), (')', 2861), ('and', 2838)]\n",
            "\n",
            "Ten most common entities: [('O', 169578), ('B-LOC', 7140), ('B-PER', 6600), ('B-ORG', 6321), ('I-PER', 4528), ('I-ORG', 3704), ('B-MISC', 3438), ('I-LOC', 1157), ('I-MISC', 1155)]\n"
          ]
        }
      ],
      "source": [
        "word_counts = Counter(words)\n",
        "entity_counts = Counter(entities)\n",
        "\n",
        "print('Ten most common words:', word_counts.most_common(10))\n",
        "print()\n",
        "print('Ten most common entities:', entity_counts.most_common(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SpaCy Pre-trained Model for NER"
      ],
      "metadata": {
        "id": "z-M-M207KRtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SpaCy provides powerful pretrained models for a wide range of NLP tasks, including Named Entity Recognition (NER). These models are trained on large annotated corpora and can recognize common entity types such as `PERSON`, `ORG`, `GPE`, `DATE`, and more.\n",
        "\n",
        "In this project, we use `en_core_web_trf`, SpaCy’s transformer-based English model. It leverages transformer architectures (like BERT) under the hood for better accuracy on complex language patterns.\n",
        "\n",
        "Advantages of using a SpaCy pretrained NER model:\n",
        "- **Out-of-the-box performance**: No training required — just load and run.\n",
        "- **Fast and efficient**: Optimized for performance even with deep models.\n",
        "- **Transfer learning**: Uses contextual embeddings from large language models.\n",
        "\n",
        "However, these models may produce entities not aligned with custom tag sets (e.g., the CoNLL-2003 `PER`, `LOC`, `ORG`, `MISC`). To address this, we map SpaCy's entity labels to the CoNLL schema before evaluation."
      ],
      "metadata": {
        "id": "tQxlZWhELPX3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D163GVODXdVX"
      },
      "source": [
        "### Quick Experiment with the spaCy Transformer Model\n",
        "\n",
        "Before building our own named entity recognizer, we run a quick experiment using spaCy’s pretrained transformer model (`en_core_web_trf`). This allows us to compare its predictions against our custom CRF model later and evaluate how well out-of-the-box solutions perform on our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7EywjnuLV86n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e884b31-03e1-4993-b926-c88ac9e25c16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities:\n",
            "Apple Inc. -> ORG\n",
            "Steve Jobs -> PERSON\n",
            "Cupertino -> GPE\n",
            "California -> GPE\n",
            "\n",
            "Execution Time: 0.2668280601501465\n"
          ]
        }
      ],
      "source": [
        "text = 'Apple Inc. was founded by Steve Jobs and is headquartered in Cupertino, California.'\n",
        "\n",
        "start_time = time.time()\n",
        "doc = nlp(text)\n",
        "end_time = time.time()\n",
        "\n",
        "print('Entities:')\n",
        "for ent in doc.ents:\n",
        "  print(f'{ent.text} -> {ent.label_}')\n",
        "\n",
        "print()\n",
        "print('Execution Time:', end_time - start_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMkNtwmQdgr3"
      },
      "source": [
        "### spaCy Model Evaluation Function\n",
        "\n",
        "This function evaluates the pretrained spaCy NER model on our dataset by aligning its predicted entity spans with the gold BIO-tagged labels. Because spaCy outputs entity spans without BIO tags, we manually convert them using character alignment and label mapping. This allows for a fair comparison between the spaCy model and the BIO-formatted ground truth.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "meAhNDJqdkgz"
      },
      "outputs": [],
      "source": [
        "def evaluate_spacy_ner(tagged_sents, label_map):\n",
        "  \"\"\"\n",
        "  Evaluates spaCy's pretrained NER model against a BIO-tagged dataset.\n",
        "\n",
        "  This function converts spaCy's entity span predictions into BIO format\n",
        "  and aligns them with the true labels from the dataset.\n",
        "\n",
        "  Args:\n",
        "      tagged_sents (List[List[Tuple[str, str, str]]]):\n",
        "          A list of sentences, where each sentence is a list of (word, POS, entity) tuples.\n",
        "      label_map (Dict[str, str]):\n",
        "          A mapping from spaCy's entity labels (e.g., 'PERSON', 'ORG') to the dataset's tag scheme (e.g., 'PER', 'ORG').\n",
        "\n",
        "  Returns:\n",
        "      Tuple[List[List[str]], List[List[str]]]:\n",
        "          - `predictions`: BIO-formatted label predictions from spaCy.\n",
        "          - `true_labels`: Ground truth BIO labels from the input data.\n",
        "\n",
        "  Notes:\n",
        "      - Assumes each sentence is tokenized and the gold labels follow IOB format.\n",
        "      - Uses character alignment (`char_span`) to map spaCy spans to token indices.\n",
        "      - Entities not in the label map are defaulted to 'O'.\n",
        "  \"\"\"\n",
        "  predictions, true_labels = [], []\n",
        "\n",
        "  tqdm_bar = tqdm(total = len(tagged_sents), mininterval = 0)\n",
        "\n",
        "  for s, sentence in enumerate(tagged_sents):\n",
        "    words, _, entities = zip(*sentence)\n",
        "    text = ' '.join(words)\n",
        "\n",
        "    doc = nlp(text)\n",
        "    preds = ['O'] * len(words)\n",
        "\n",
        "    for ent in doc.ents:\n",
        "      span = doc.char_span(ent.start_char, ent.end_char, alignment_mode = 'expand')\n",
        "\n",
        "      if span:\n",
        "        for i, word in enumerate(words):\n",
        "          if text.find(word) == ent.start_char:\n",
        "            prediction = label_map.get(ent.label_, 'O')\n",
        "            preds[i] = f'B-{prediction}' if prediction != 'O' else 'O'\n",
        "            for j in range(1, len(span)):\n",
        "              if j + i < len(words):\n",
        "                preds[j + i] = f'I-{prediction}' if prediction != 'O' else 'O'\n",
        "            break\n",
        "\n",
        "    predictions.append(preds)\n",
        "    true_labels.append(list(entities))\n",
        "\n",
        "    if s % 250 == 0:\n",
        "      tqdm_bar.update(250)\n",
        "\n",
        "  tqdm_bar.close()\n",
        "\n",
        "  return predictions, true_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding MISC Labels and Mapping spaCy Entity Types\n",
        "\n",
        "In this section, we explore which words are commonly labeled as `MISC` in the CoNLL-2003 dataset. The `MISC` category is used for entities that don't fall under standard categories like `PER` (person), `LOC` (location), or `ORG` (organization). Examples often include nationalities (e.g., \"French\", \"Israeli\"), political affiliations, or event names like \"Cup\" or \"League\".\n",
        "\n",
        "To analyze this, we extract all words in the training set that are tagged as `B-MISC` or `I-MISC`, count their frequency, and inspect the most common examples. This helps us better understand what the dataset considers `MISC`, which is especially useful for aligning external model predictions (like spaCy's) to this tag schema.\n",
        "\n",
        "We then define a `label_map` to translate spaCy’s NER labels (e.g., `PERSON`, `GPE`, `NORP`, etc.) into CoNLL-style tags (`PER`, `LOC`, `ORG`, `MISC`, or `O`). This is important because spaCy predicts its own set of entity types, and we need a consistent label space to evaluate against the true CoNLL annotations."
      ],
      "metadata": {
        "id": "fT8Ih8ymKBBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_with_misc = []\n",
        "for i, sent in enumerate(train_sentences):\n",
        "  word, _, entity = zip(*sent)\n",
        "  for j, e in enumerate(entity):\n",
        "    if e == 'B-MISC' or e == 'I-MISC':\n",
        "      words_with_misc.append(word[j])\n",
        "\n",
        "misc_counts = Counter(words_with_misc)\n",
        "print(misc_counts.most_common(100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsSN9r0wJ3WA",
        "outputId": "ce03d396-09af-4165-ff2d-c207f175bca5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Russian', 95), ('Cup', 95), ('German', 84), ('British', 78), ('Open', 71), ('French', 67), ('European', 55), ('American', 55), ('Australian', 52), ('Iraqi', 51), ('Dutch', 50), ('Israeli', 49), ('GMT', 49), ('World', 49), ('League', 49), ('DIVISION', 48), ('African', 45), ('English', 42), ('Olympic', 41), ('LEAGUE', 41), ('Palestinian', 39), ('U.S.', 38), ('Italian', 35), ('South', 34), ('Kurdish', 33), ('Bosnian', 32), ('Moslem', 32), ('Democratic', 31), ('Belgian', 28), ('Japanese', 28), ('Serb', 28), ('Republican', 28), ('OPEN', 26), ('Chinese', 25), ('Grand', 25), ('Turkish', 24), ('Sudanese', 23), ('Polish', 22), ('Iranian', 22), ('Swiss', 22), ('C$', 21), ('Brazilian', 21), ('Indian', 21), ('Palestinians', 20), ('of', 20), ('CUP', 20), ('Canadian', 20), ('National', 20), ('Chechen', 20), ('Thai', 20), ('ENGLISH', 19), ('A$', 19), ('Wimbledon', 18), ('Serbs', 18), ('GERMAN', 16), ('Series', 16), ('MAJOR', 16), ('Major', 16), ('EASTERN', 16), ('CENTRAL', 16), ('WESTERN', 16), ('Democrats', 16), ('Albanian', 16), ('Slovak', 16), ('War', 15), ('Korean', 15), ('Jewish', 15), ('Windows', 15), ('Afghan', 14), ('DUTCH', 14), ('Austrian', 14), ('Argentine', 14), ('Arab', 14), ('Spanish', 13), ('Greek', 13), ('Islamic', 13), ('Prix', 13), ('RBI', 13), ('Tour', 13), ('FRENCH', 13), ('Sri', 13), ('Nicaraguan', 13), ('Saudi', 12), (\"'\", 12), ('Algerian', 12), ('Irish', 12), ('Yugoslav', 12), ('Islamist', 12), ('95', 12), ('Nations', 12), ('BSE', 11), ('Scottish', 11), ('Western', 11), ('Slam', 11), ('Rwandan', 11), ('1,000', 11), ('Serbian', 10), ('Catholic', 10), ('Day', 10), ('Pakistani', 10)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7TnD-7G_Arfe"
      },
      "outputs": [],
      "source": [
        "label_map = {\n",
        "    'PERSON': 'PER',\n",
        "    'GPE': 'LOC',\n",
        "    'ORG': 'ORG',\n",
        "    'NORP': 'MISC',\n",
        "    'FAC': 'MISC',\n",
        "    'EVENT': 'MISC',\n",
        "    'WORK_OF_ART': 'O',\n",
        "    'LAW': 'O',\n",
        "    'LANGUAGE': 'MISC',\n",
        "    'PRODUCT': 'MISC',\n",
        "    'DATE': 'O',\n",
        "    'TIME': 'O',\n",
        "    'PERCENT': 'O',\n",
        "    'MONEY': 'O',\n",
        "    'QUANTITY': 'O',\n",
        "    'ORDINAL': 'O',\n",
        "    'CARDINAL': 'O'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation of spaCy NER Predictions\n",
        "\n",
        "We apply the `evaluate_spacy_ner` function to generate BIO-formatted entity predictions using the spaCy transformer-based NER model. The predictions are aligned to the CoNLL-2003 label set using our custom `label_map`.\n",
        "\n",
        "The resulting predictions are then evaluated using `classification_report` from `sklearn`, which provides precision, recall, and F1-score metrics for each entity class (`PER`, `LOC`, `ORG`, `MISC`). This helps us assess how well the spaCy model aligns with the ground truth annotations in the test set."
      ],
      "metadata": {
        "id": "Z0x_gN3aKMuQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4ejhOPjjPL8",
        "outputId": "3d936b95-8124-43b8-8f00-a7cdd071eea8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "6750it [15:22,  7.32it/s]\n"
          ]
        }
      ],
      "source": [
        "pred, true_labels = evaluate_spacy_ner(test_sentences, label_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Efi06KMKmkuM",
        "outputId": "383e75f5-670a-4658-efca-a1b8dcd52577"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.78      0.79      0.78      3505\n",
            "        MISC       0.77      0.65      0.70      1624\n",
            "         ORG       0.76      0.42      0.54      3002\n",
            "         PER       0.88      0.89      0.89      3459\n",
            "\n",
            "   micro avg       0.81      0.70      0.75     11590\n",
            "   macro avg       0.80      0.69      0.73     11590\n",
            "weighted avg       0.80      0.70      0.74     11590\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(true_labels, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OMg9jTRZ8Yx"
      },
      "source": [
        "## CRF-Based NER\n",
        "\n",
        "Conditional Random Fields (CRFs) are a type of probabilistic sequence modeling algorithm commonly used for tasks like Named Entity Recognition (NER) and Part-of-Speech tagging.\n",
        "\n",
        "Unlike classifiers that make independent predictions for each word, CRFs model the sequence of labels jointly, capturing dependencies between neighboring tags. This is particularly useful in NER, where tag consistency matters (e.g., an `I-PER` tag must follow a `B-PER`).\n",
        "\n",
        "In the context of NER, CRFs learn the most likely tag sequence for a sentence by using features such as:\n",
        "- The current word and surrounding words\n",
        "- Part-of-speech tags\n",
        "- Word shape (e.g., capitalization, digits, symbols)\n",
        "- Prefixes/suffixes\n",
        "\n",
        "CRFs are effective because they:\n",
        "- Consider the context of the entire sentence\n",
        "- Enforce valid tag transitions\n",
        "- Support rich feature engineering\n",
        "\n",
        "This makes them well-suited for structured prediction tasks involving sequences of text."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Shape Feature Extraction\n",
        "\n",
        "The `get_word_shape` function generates a simplified representation of a word's character pattern, known as **word shape**. This is a common feature in sequence labeling tasks like NER to help models generalize across words with similar patterns.\n",
        "\n",
        "Each character is mapped as follows:\n",
        "- `'X'` for uppercase letters\n",
        "- `'x'` for lowercase letters\n",
        "- `'d'` for digits\n",
        "- `'s'` for symbols or punctuation\n",
        "\n",
        "Repeated character types are **collapsed** using a regex to reduce redundancy. For example:\n",
        "- `\"Chicken\"` becomes `\"Xxxxxxx\"` → `\"Xx\"`\n",
        "- `\"9-ball\"` becomes `\"d-xxxxx\"` → `\"dx\"`\n",
        "- `\"NoMa'am!\"` becomes `\"XxXx'x!\"` → `\"XxXs\"`\n",
        "\n",
        "This shape helps the model recognize structure in unknown or rare words."
      ],
      "metadata": {
        "id": "2EnoOOYEqZg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_shape(word):\n",
        "  shape = ''\n",
        "  for char in word:\n",
        "    if char.isupper():\n",
        "      shape += 'X'\n",
        "    elif char.islower():\n",
        "      shape += 'x'\n",
        "    elif char.isdigit():\n",
        "      shape += 'd'\n",
        "    else:\n",
        "      shape += 's'\n",
        "\n",
        "  shape = re.sub(r'(.)\\1+', r'\\1', shape)\n",
        "  return shape"
      ],
      "metadata": {
        "id": "Rh66criRopwA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Examples of Word Shapes:')\n",
        "print('------------------------')\n",
        "print('Chicken:', get_word_shape('Chicken'))\n",
        "print('9-ball:', get_word_shape('9-ball'))\n",
        "print(\"NoMa'am!:\", get_word_shape(\"NoMa'am!\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7-4geRWpWQM",
        "outputId": "12f90ae2-9685-47ea-a99b-fcf9dbb67c37"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Examples of Word Shapes:\n",
            "------------------------\n",
            "Chicken: Xx\n",
            "9-ball: dsx\n",
            "NoMa'am!: XxXxsxs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Engineering for CRF-based NER\n",
        "\n",
        "To train a CRF model for Named Entity Recognition, we need to extract rich, informative features for each word in a sentence. These features help the model capture local context and structural patterns relevant for entity labeling.\n",
        "\n",
        "The `create_word_features` function constructs a dictionary of features for a given word in a sentence. These include:\n",
        "\n",
        "- **Current word features:**\n",
        "  - `word.lower()`: Lowercased form of the word\n",
        "  - `word[-3:]`: Last 3 characters (suffix)\n",
        "  - `word[3:]`: All but the first 3 characters (prefix)\n",
        "  - `word.isupper()`, `word.islower()`, `word.istitle()`: Casing indicators\n",
        "  - `word.isdigit()`: Whether the token is numeric\n",
        "  - `word.shape`: Custom word shape (e.g., `'Xx'` for 'Chicken', `'d-xx'` for '9-ball')\n",
        "  - `pos_tag`: The part-of-speech tag\n",
        "  - `pos_tag[:2]`: The first two characters of the POS tag (to capture coarse tag category)\n",
        "\n",
        "- **Previous and next word features:**\n",
        "  - Lowercase word, POS tag, and shape of the previous and next tokens\n",
        "  - Special flags for beginning (`BOS`) and end (`EOS`) of sentence\n",
        "\n",
        "The `create_sentence_features` function applies this logic across a full sentence, producing a sequence of token-level feature dictionaries. The `get_labels` function extracts the true IOB entity tags from a sentence.\n",
        "\n",
        "This feature-based approach provides the necessary structure for training the CRF to detect named entities using both lexical and contextual cues."
      ],
      "metadata": {
        "id": "SvRy1YPAqbJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_word_features(sentence, i):\n",
        "  \"\"\"\n",
        "  Extracts a set of contextual and lexical features for a word at position `i` in a sentence.\n",
        "\n",
        "  Args:\n",
        "      sentence (List[Tuple[str, str]]): A list of (word, POS) tuples representing a sentence.\n",
        "      i (int): Index of the target word in the sentence.\n",
        "\n",
        "  Returns:\n",
        "      dict: A dictionary of features for the CRF model, including:\n",
        "          - Lexical features: word lowercased, suffix/prefix, casing, digit, shape\n",
        "          - POS tag and first two characters of the tag\n",
        "          - Features of previous and next word (if available)\n",
        "          - Special flags for beginning (`BOS`) or end (`EOS`) of sentence\n",
        "  \"\"\"\n",
        "  word, pos_tag = sentence[i][0], sentence[i][1]\n",
        "\n",
        "  features = {\n",
        "      'bias': 1.0,\n",
        "      'word.lower()': word.lower(),\n",
        "      'word[-3:]': word[-3:],\n",
        "      'word[3:]': word[3:],\n",
        "      'word.isupper()': word.isupper(),\n",
        "      'word.islower()': word.islower(),\n",
        "      'word.istitle()': word.istitle(),\n",
        "      'word.isdigit()': word.isdigit(),\n",
        "      'word.shape': get_word_shape(word),\n",
        "      'pos_tag': pos_tag,\n",
        "      'pos_tag[:2]': pos_tag[:2]\n",
        "  }\n",
        "\n",
        "  if i > 0:\n",
        "    prev_word, prev_pos_tag = sentence[i - 1][0], sentence[i - 1][1]\n",
        "    features.update({\n",
        "        '-1:word.lower()': prev_word.lower(),\n",
        "        '-1:pos_tag': prev_pos_tag,\n",
        "        '-1:shape': get_word_shape(prev_word)\n",
        "    })\n",
        "  else:\n",
        "    features['BOS'] = True\n",
        "\n",
        "  if i < len(sentence) - 1:\n",
        "    next_word, next_pos_tag = sentence[i + 1][0], sentence[i + 1][1]\n",
        "    features.update({\n",
        "        '+1:word.lower()': next_word.lower(),\n",
        "        '+1:pos_tag': next_pos_tag,\n",
        "        '+1:word.shape': get_word_shape(next_word)\n",
        "    })\n",
        "  else:\n",
        "    features['EOS'] = True\n",
        "\n",
        "  return features"
      ],
      "metadata": {
        "id": "n-Xl5xirpXPE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sentence_features(sentence):\n",
        "  \"\"\"\n",
        "  Generates a list of feature dictionaries for each word in a sentence.\n",
        "\n",
        "  Args:\n",
        "      sentence (List[Tuple[str, str, str]]): A list of (word, POS, label) tuples.\n",
        "\n",
        "  Returns:\n",
        "      List[Dict[str, Any]]: A list of dictionaries, each containing features for a single word.\n",
        "                            Features are generated using the `create_word_features` function.\n",
        "  \"\"\"\n",
        "  return [create_word_features(sentence, i) for i in range(len(sentence))]\n",
        "\n",
        "def get_labels(sentence):\n",
        "  \"\"\"\n",
        "  Extracts the label sequence from a tagged sentence.\n",
        "\n",
        "  Args:\n",
        "      sentence (List[Tuple[str, str, str]]): A list of (word, POS, label) tuples.\n",
        "\n",
        "  Returns:\n",
        "      List[str]: A list of labels corresponding to each token in the sentence.\n",
        "  \"\"\"\n",
        "  return [label for _, _, label in sentence]"
      ],
      "metadata": {
        "id": "rtNiXvpur3vY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the CRF Model\n",
        "\n",
        "To train our Conditional Random Field (CRF) model for named entity recognition, we first transform our training data into feature dictionaries using the `create_sentence_features` function. These features include word characteristics, surrounding context, part-of-speech tags, and shape-based indicators.\n",
        "\n",
        "We use the `sklearn-crfsuite` implementation of CRF with the following settings:\n",
        "- `algorithm='lbfgs'`: A quasi-Newton optimization method.\n",
        "- `c1=0.1`, `c2=0.1`: Regularization parameters to prevent overfitting.\n",
        "- `max_iterations=100`: Maximum number of iterations for convergence.\n",
        "- `all_possible_transitions=True`: Allows the model to consider transitions that may not appear in the training set, improving generalization.\n",
        "\n",
        "This step fits the CRF model on the training data and learns transition and emission parameters for predicting BIO-tagged entities."
      ],
      "metadata": {
        "id": "XHvCI01JsO4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = [create_sentence_features(sentence) for sentence in train_sentences]\n",
        "y_train = [get_labels(sentence) for sentence in train_sentences]"
      ],
      "metadata": {
        "id": "Lxlj8LmcsMoQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crf_model = CRF(\n",
        "    algorithm = 'lbfgs',\n",
        "    c1 = 0.1,\n",
        "    c2 = 0.1,\n",
        "    max_iterations = 100,\n",
        "    all_possible_transitions = True\n",
        ")"
      ],
      "metadata": {
        "id": "DQlqi-x2sYSg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crf_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "id": "4aIeeqH3smcI",
        "outputId": "26d7483a-a48c-49c6-c2bc-f06361b93f87"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n",
              "    max_iterations=100)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=True, c1=0.1, c2=0.1,\n",
              "    max_iterations=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>CRF</div></div><div><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=True, c1=0.1, c2=0.1,\n",
              "    max_iterations=100)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the CRF Model\n",
        "\n",
        "After training, we evaluate the CRF model on the test set. The test sentences are first converted into feature dictionaries using the same `create_sentence_features` function used during training.\n",
        "\n",
        "We then:\n",
        "- Generate predictions using the trained `crf_model`.\n",
        "- Use `classification_report` from `seqeval` to compute standard NER evaluation metrics such as precision, recall, and F1-score for each entity class.\n",
        "\n",
        "This allows us to assess how well the CRF model is able to generalize to unseen data based on the BIO-tagged ground truth.\n",
        "\n"
      ],
      "metadata": {
        "id": "NbmRj90KtkFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = [create_sentence_features(sentence) for sentence in test_sentences]\n",
        "y_test = [get_labels(sentence) for sentence in test_sentences]"
      ],
      "metadata": {
        "id": "omrNQKT7snz1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = crf_model.predict(X_test)"
      ],
      "metadata": {
        "id": "_NHu5gVZtq-z"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytKLatdztt8z",
        "outputId": "e7ecc98f-2c5e-42e2-9751-1ebcadd57720"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.90      0.90      0.90      3505\n",
            "        MISC       0.86      0.81      0.84      1624\n",
            "         ORG       0.82      0.77      0.79      3002\n",
            "         PER       0.88      0.88      0.88      3459\n",
            "\n",
            "   micro avg       0.87      0.85      0.86     11590\n",
            "   macro avg       0.86      0.84      0.85     11590\n",
            "weighted avg       0.86      0.85      0.86     11590\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the CRF model performs better than the SpaCy pre-trained model. This is likely due to the fact that we trained it from scratch on the labels we see in our dataset. The SpaCy model's outputs do not agree with our dataset and so we needed to adjust them manually, leading to worse performance."
      ],
      "metadata": {
        "id": "at1IOYqDMMKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Tuning with RandomizedSearchCV\n",
        "\n",
        "To optimize the CRF model's performance, we perform hyperparameter tuning using `RandomizedSearchCV`. Specifically, we tune the `c1` and `c2` regularization parameters, which control L1 and L2 regularization strength respectively. Both are sampled from a log-uniform distribution between 0.01 and 1.\n",
        "\n",
        "We define a custom scoring function using `flat_f1_score` with weighted averaging to ensure class imbalance is considered. A 3-fold cross-validation is used during the search.\n",
        "\n",
        "After identifying the best hyperparameters, we evaluate the best estimator on the test set using `classification_report` to get detailed precision, recall, and F1-score per entity class.\n",
        "\n",
        "This approach helps balance model complexity and generalization, improving performance on unseen data."
      ],
      "metadata": {
        "id": "ESUx2TD9t5SJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crf_model = CRF(\n",
        "    algorithm = 'lbfgs',\n",
        "    max_iterations = 100,\n",
        "    all_possible_transitions = True\n",
        ")"
      ],
      "metadata": {
        "id": "EyLQE6NrtxcA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_space = {\n",
        "    'c1': loguniform(0.01, 1),\n",
        "    'c2': loguniform(0.01, 1)\n",
        "}\n",
        "\n",
        "from sklearn_crfsuite import CRF\n",
        "\n",
        "def custom_score(estimator, X, y):\n",
        "  y_pred = estimator.predict(X)\n",
        "  return flat_f1_score(y, y_pred, average = 'weighted')"
      ],
      "metadata": {
        "id": "4NQNfFIvuyN7"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crf_grid = RandomizedSearchCV(\n",
        "    crf_model,\n",
        "    param_space,\n",
        "    scoring = custom_score,\n",
        "    cv = 3,\n",
        "    verbose = 3,\n",
        "    error_score = 'raise'\n",
        ")\n",
        "\n",
        "crf_grid.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ2KWq3Wu5Ug",
        "outputId": "d579e1b1-a890-4f51-bbdd-6a449f25280c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "[CV 1/3] END c1=0.16061952962331363, c2=0.8808514875228187;, score=0.968 total time=  22.0s\n",
            "[CV 2/3] END c1=0.16061952962331363, c2=0.8808514875228187;, score=0.970 total time=  23.8s\n",
            "[CV 3/3] END c1=0.16061952962331363, c2=0.8808514875228187;, score=0.966 total time=  23.1s\n",
            "[CV 1/3] END c1=0.011038639969250728, c2=0.2291981564176078;, score=0.971 total time=  22.4s\n",
            "[CV 2/3] END c1=0.011038639969250728, c2=0.2291981564176078;, score=0.974 total time=  24.1s\n",
            "[CV 3/3] END c1=0.011038639969250728, c2=0.2291981564176078;, score=0.969 total time=  21.9s\n",
            "[CV 1/3] END c1=0.3560044300530145, c2=0.017454558292561765;, score=0.969 total time=  22.1s\n",
            "[CV 2/3] END c1=0.3560044300530145, c2=0.017454558292561765;, score=0.971 total time=  23.5s\n",
            "[CV 3/3] END c1=0.3560044300530145, c2=0.017454558292561765;, score=0.968 total time=  23.7s\n",
            "[CV 1/3] END c1=0.023377805507015053, c2=0.024281216631669565;, score=0.971 total time=  22.8s\n",
            "[CV 2/3] END c1=0.023377805507015053, c2=0.024281216631669565;, score=0.974 total time=  23.1s\n",
            "[CV 3/3] END c1=0.023377805507015053, c2=0.024281216631669565;, score=0.969 total time=  22.2s\n",
            "[CV 1/3] END c1=0.023290430807422647, c2=0.2831212720582784;, score=0.971 total time=  21.8s\n",
            "[CV 2/3] END c1=0.023290430807422647, c2=0.2831212720582784;, score=0.974 total time=  25.0s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best params:', crf_grid.best_params_)\n",
        "print('Best score:', crf_grid.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PT3oHlC6wDC9",
        "outputId": "918de54f-c061-4c35-989d-0fd06dda0645"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params: {'c1': np.float64(0.023377805507015053), 'c2': np.float64(0.024281216631669565)}\n",
            "Best score: 0.9716806319568848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_crf = crf_grid.best_estimator_\n",
        "y_pred = best_crf.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5J-YzamvwLwa",
        "outputId": "aae430e1-30a5-4747-9295-fd375dbcdf57"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.89      0.91      0.90      3505\n",
            "        MISC       0.86      0.82      0.84      1624\n",
            "         ORG       0.82      0.76      0.79      3002\n",
            "         PER       0.88      0.89      0.88      3459\n",
            "\n",
            "   micro avg       0.87      0.85      0.86     11590\n",
            "   macro avg       0.86      0.85      0.85     11590\n",
            "weighted avg       0.86      0.85      0.86     11590\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final performance of the model doesn't improve much with further hyperparameter tuning. In future experiments we could explore different features to improve performance."
      ],
      "metadata": {
        "id": "K7NnlXLzMqP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion: CRF vs. SpaCy\n",
        "\n",
        "The CRF-based Named Entity Recognition model outperforms the pre-trained SpaCy transformer model on this dataset. While SpaCy leverages general-purpose language representations, the CRF is trained specifically on the CoNLL-2003 data, allowing it to learn fine-grained sequence patterns and label dependencies. The CRF's ability to incorporate handcrafted features like word shape, part-of-speech tags, and surrounding context helps it achieve higher precision and recall, particularly for challenging entity types like `MISC` and `ORG`.\n",
        "\n",
        "This highlights the strength of CRFs in structured prediction tasks where domain-specific patterns and label consistency are crucial."
      ],
      "metadata": {
        "id": "arTpeNtuM1yv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DYQ-hvmWIcxG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}