# Week 2 ‚Äì Named Entity Recognition

## üìå Objective
This project compares two NER approaches on the **CoNLL-2003** dataset:

1. **CRF Model** ‚Äì Uses handcrafted lexical, POS, and neighboring token features, plus BOS/EOS flags, to learn BIO tagging.  
2. **spaCy Transformer (`en_core_web_trf`)** ‚Äì Generates entity spans, maps them to CoNLL labels, and converts to BIO format for evaluation.

**Goal:**  
Evaluate both models on the same test set and compare a feature-based CRF with a pre-trained transformer.

## üß© Skills & Concepts
- **Named Entity Recognition (NER)** using both statistical (CRF) and transformer-based models.
- **BIO Tagging Scheme** for entity span labeling.
- **Feature Engineering** for CRFs:
  - Token shape
  - POS tags and prefixes
  - Contextual features from neighboring tokens
  - BOS/EOS indicators
- **Entity Mapping** from spaCy labels to CoNLL categories.
- **Data Preprocessing** for CoNLL-2003 format.
- **Model Evaluation** with precision, recall, and F1 scores.

## üì¶ Dataset
- **Source:** [CoNLL-2003 Dataset](https://www.kaggle.com/datasets/juliangarratt/conll2003-dataset)
- **Size:** 14,041 training sentences and 6,703 testing sentences
- **Preprocessing:** Minimal preprocessing was done for this project. Sentences that were empty were removed from the dataset. 

Below is an example from the train dataset: 

[('EU', 'NNP', 'B-ORG'), ('rejects', 'VBZ', 'O'), ('German', 'JJ', 'B-MISC'), ('call', 'NN', 'O'), ('to', 'TO', 'O'), ('boycott', 'VB', 'O'), ('British', 'JJ', 'B-MISC'), ('lamb', 'NN', 'O'), ('.', '.', 'O')]

The dataset contains token-level labels in the BIO format for four entity types:

- **PER** ‚Äì Person names
- **LOC** ‚Äì Locations
- **ORG** ‚Äì Organizations
- **MISC** ‚Äì Miscellaneous entities

Each sentence is annotated with word, POS tag, and entity label.  We only focus on the entity label for this project. 

## üìÇ Project Structure
- `src/` ‚Äì Core source code and utilities for creating and evaluating the SpaCy and CRF models
- `notebooks/` ‚Äì Prototyping and data exploration  
- `results/` - Contains performance metrics for the CRF and SpaCy models evaluated on the test set
- `main.py` ‚Äì Script to run models and log metrics
- `requirements.txt` ‚Äì Dependencies  

## ‚öôÔ∏è Setup
```bash
# Create environment
conda create -n week2 python=3.10 -y
conda activate week2

# Install dependencies
pip install -r requirements.txt
```

## üöÄ How to Run 
```bash 
python3 main.py
```

## üìä Results
We evaluated both a Conditional Random Field (CRF) model and SpaCy‚Äôs `en_core_web_trf` transformer-based NER model on the **CoNLL-2003** dataset.  
Metrics are reported as **F1-scores** for each entity type and aggregated averages.  

The CRF model outperformed SpaCy in most categories, particularly for `ORG` and `MISC`, while SpaCy had a slight edge on `PER`.

| Label        | CRF F1  | SpaCy F1 | Œî F1 (CRF ‚àí SpaCy) |
|--------------|---------|----------|--------------------|
| LOC          | 0.897   | 0.783    | **+0.114**         |
| MISC         | 0.835   | 0.703    | **+0.132**         |
| ORG          | 0.792   | 0.540    | **+0.252**         |
| PER          | 0.880   | 0.889    | -0.009             |
| **Micro Avg**| 0.857   | 0.754    | **+0.103**         |
| **Macro Avg**| 0.851   | 0.729    | **+0.122**         |
| **Weighted** | 0.856   | 0.741    | **+0.115**         |


## üìå Key Takeaways
- **CRF consistently outperforms SpaCy** across most entity types, with the largest improvements in `ORG` (+25.2 F1) and `MISC` (+13.2 F1).  
- **SpaCy only slightly leads on `PER`** (+0.9 F1), likely benefiting from its transformer-based contextual embeddings for personal names.  
- **Feature engineering in CRF remains competitive**, especially for domains with limited or domain-specific training data.  
- The large gap in `ORG` suggests that SpaCy‚Äôs general-purpose model struggles with certain organization names without domain-specific fine-tuning.  
- **Label translation was required** for SpaCy predictions (e.g., `GPE ‚Üí LOC`, `NORP ‚Üí MISC`), introducing potential inconsistencies‚Äîespecially where CoNLL uses a single label for multiple semantic types (e.g., `MISC`).  
- **Macro and weighted averages** both favor CRF by over 12 F1 points, showing stronger overall performance despite translation challenges.  

## üß† Engineering Notes
- **Models:**
  - **CRF:** Hand-crafted lexical, orthographic, POS, and context features.
  - **SpaCy (`en_core_web_trf`):** Transformer-based NER without fine-tuning.

- **Label Mapping:**
  - SpaCy outputs mapped to CoNLL-2003 tags (e.g., `GPE ‚Üí LOC`, `NORP ‚Üí MISC`).
  - Some categories (e.g., `MISC`) are broad, leading to potential misalignments.

- **Processing:**
  - CoNLL-2003 dataset (Kaggle) in BIO format.
  - SpaCy predictions converted to BIO for evaluation.
  - JSON outputs required converting NumPy types to native Python.

- **Environment:**
  - Python 3.10, local execution.
  - Transformer model on CPU; CRF training fast due to lightweight features.

## üó∫Ô∏è Next Steps
- Fine-tune the SpaCy transformer model on the CoNLL-2003 dataset to improve performance.  
- Refine the label mapping to better align SpaCy outputs with CoNLL tags and reduce inconsistencies.  
- Enhance CRF features by adding character n-grams, embeddings, and other lexical signals.  
- Expand evaluation to include confusion matrices and span-level F1 scores.  
- Package both models for easy inference and side-by-side evaluation on new data.

## üîó References
- Jurafsky, D., & Martin, J. H. *Speech and Language Processing* ‚Äì Chapters on sequence labeling (HMMs, CRFs) and topic modeling.  
- Bird, S., Klein, E., & Loper, E. *Natural Language Processing with Python* ‚Äì CRFsuite examples.  
- Stanford CS124 and CS224N archived lectures (older editions with classical tagging content).  

