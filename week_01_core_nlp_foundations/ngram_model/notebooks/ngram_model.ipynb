{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ws1FmtLV4JGl"
   },
   "source": [
    "# N-Gram Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSkMxuDXdgp4"
   },
   "source": [
    "In this notebook, we implement a classical n-gram language model from scratch using the Penn Treebank dataset. The goal is to build unigram, bigram, and trigram models, generate text, and evaluate model quality using **perplexity**. We also explore **Laplace smoothing** and **back-off models** to handle unseen sequences in test data.\n",
    "\n",
    "Key concepts demonstrated:\n",
    "- Tokenization and sentence boundary handling\n",
    "- N-gram construction and probability estimation\n",
    "- Text generation using learned n-gram models\n",
    "- Perplexity-based evaluation\n",
    "- Smoothing techniques to improve generalization\n",
    "\n",
    "This notebook is part of my NLP learning journey (Week 1 – Core NLP Foundations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WBfdHI74LZl"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install needed libraries\n",
    "!pip install nltk\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk packages\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Z_4YNz14oGS"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHqHq4Yq79sK"
   },
   "source": [
    "First, we are going to load our dataset from HuggingFace. I've selected the Penn Treebank Project: Release 2 CDROM as the dataset for this exercise. The Penn Treebank Project contains excerpts from the 1989 Wall Street Journal. The rare words are already replaced with an <UNK> token and the numbers are replaced with a placeholder token, making this a good introductory dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1820a51222914429ba50256987cce6b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f27daeb6b80483abce6a891e1c89b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ptb_text_only.py:   0%|          | 0.00/6.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository for ptb_text_only contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/ptb_text_only.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N] y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec6a974e6739484e82eb712b767b6f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/5.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53733819aba349cb9509722c93195b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/400k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972f6afbba17404f979196b781e0fecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/450k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d960880f0d948968726bf7bf5862a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/42068 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44fd44425fd4e5f85eb8b17a3ef9b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3761 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9faaa5f09ba45529c39fb8dd99f6f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3370 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_data = load_dataset('ptb_text_only', split = 'train')\n",
    "text_data = pd.DataFrame(text_data).values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwwFAIFO6Hy4"
   },
   "source": [
    "## Split into Train / Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmqkK-uH8jw1"
   },
   "source": [
    "In order to train and evaluate our n-gram models, we need to split the dataset into training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(text_data, test_size = 0.2, random_state = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXiT3ngs6ORC"
   },
   "source": [
    "## Preprocess Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SthZh8mM8uhN"
   },
   "source": [
    "Since this dataset is already highly preprocessed, we will perform a simple tokenization by splitting on whitespace. We also append a start token, `<s>`, and an end token, `</s>`, to the beginning and end of each sentence. These tokens help the n-gram models learn to distinguish betweeen the start and end of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ptb(series):\n",
    "  tokenized_sentences = []\n",
    "  for text in series:\n",
    "    tokens = text[0].split()\n",
    "    tokens = ['<s>'] + tokens + ['</s>']\n",
    "    tokenized_sentences.append(tokens)\n",
    "  return [token for sentence in tokenized_sentences for token in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: ['in <unk> an important swing area republican <unk> now run on a <unk> promising to keep the county clean and green']\n",
      "Preprocessed Sentence: ['<s>', 'in', '<unk>', 'an', 'important', 'swing', 'area', 'republican', '<unk>', 'now', 'run', 'on', 'a', '<unk>', 'promising', 'to', 'keep', 'the', 'county', 'clean', 'and', 'green', '</s>']\n"
     ]
    }
   ],
   "source": [
    "check = train_data[809]\n",
    "print('Original Sentence:', check)\n",
    "print('Preprocessed Sentence:', preprocess_ptb([check]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = preprocess_ptb(train_data)\n",
    "test_tokens = preprocess_ptb(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLxRuJRi7SbB"
   },
   "source": [
    "## Generate N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dROn0cYf-EzN"
   },
   "source": [
    "Now we can define our functions for building our n-gram models. First, we need to define a function to generate n-grams.\n",
    "\n",
    "N-grams are sequences of tokens that appear together in the corpus. We use the n-grams to train our probabilistic model. For example, if we have the sentence,\"the dog wagged his tail\", we can extract the following bigrams: ('the', 'dog'), ('dog', 'wagged'), ('wagged', 'his'), ('his', 'tail')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(tokens, n):\n",
    "  '''\n",
    "  Generates n-grams from a list of tokens.\n",
    "\n",
    "  Args:\n",
    "    - tokens (List[str]): A list of string tokens.\n",
    "    - n (int): The number of items in each n-gram (e.g., 2 for bigrams).\n",
    "\n",
    "  Returns:\n",
    "    - List[Tuple[str]]: A list of n-gram tuples.\n",
    "\n",
    "  Example:\n",
    "    >>> generate_ngrams([\"the\", \"dog\", \"wagged\", \"his\", \"tail\"], 2)\n",
    "    [('the', 'dog'), ('dog', 'wagged'), ('wagged', 'his'), ('his', 'tail')]\n",
    "  '''\n",
    "  return [tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPpgnJvv839T"
   },
   "source": [
    "## Generate Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKqul8qTDz4z"
   },
   "source": [
    "To build our n-gram models, we need to calculate the probability of each n-gram based on our training corpus.\n",
    "\n",
    "Suppose our entire corpus contains a single sentence:  \n",
    "**\"the man went to the store\"**\n",
    "\n",
    "The **bigrams** extracted from this sentence would be:\n",
    "\n",
    "('the', 'man'), ('man', 'went'), ('went', 'to'), ('to', 'the'), ('the', 'store')\n",
    "\n",
    "\n",
    "Now, to compute the probability of the bigram **('the', 'store')**, we use the following formula:\n",
    "\n",
    "P('store' | 'the') = Count('the', 'store') / Count('the')\n",
    "\n",
    "\n",
    "In this case:\n",
    "- `'the'` appears **twice** in the corpus\n",
    "- The bigram `('the', 'store')` appears **once**\n",
    "\n",
    "So the probability is:\n",
    "\n",
    "P('store' | 'the') = 1 / 2 = 0.5\n",
    "\n",
    "This method is used to estimate the likelihood of a word given its preceding context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unigram_model(tokens):\n",
    "  \"\"\"\n",
    "  Builds a unigram (1-gram) language model from a list of tokens.\n",
    "\n",
    "  Args:\n",
    "      tokens (List[str]): A list of word tokens.\n",
    "\n",
    "  Returns:\n",
    "      Dict[str, float]: A dictionary mapping each word to its probability\n",
    "      (frequency normalized by the total number of tokens).\n",
    "  \"\"\"\n",
    "  unigram_counts = Counter(tokens)\n",
    "  denominator = len(tokens)\n",
    "  return {\n",
    "      word: unigram_counts[word] / denominator\n",
    "      for word in unigram_counts\n",
    "  }\n",
    "\n",
    "def build_bigram_model(tokens):\n",
    "  \"\"\"\n",
    "  Builds a bigram (2-gram) language model from a list of tokens.\n",
    "\n",
    "  Args:\n",
    "      tokens (List[str]): A list of word tokens.\n",
    "\n",
    "  Returns:\n",
    "      Dict[Tuple[str, str], float]: A dictionary mapping bigrams to their conditional\n",
    "      probabilities P(w2 | w1), computed as count(w1, w2) / count(w1).\n",
    "  \"\"\"\n",
    "  bigram_counts = Counter(generate_ngrams(tokens, 2))\n",
    "  unigram_counts = Counter(tokens)\n",
    "  return {\n",
    "      (w1, w2): bigram_counts[(w1, w2)] / unigram_counts[w1]\n",
    "      for (w1, w2) in bigram_counts\n",
    "  }\n",
    "\n",
    "def build_trigram_model(tokens):\n",
    "  \"\"\"\n",
    "  Builds a trigram (3-gram) language model from a list of tokens.\n",
    "\n",
    "  Args:\n",
    "      tokens (List[str]): A list of word tokens.\n",
    "\n",
    "  Returns:\n",
    "      Dict[Tuple[str, str, str], float]: A dictionary mapping trigrams to their conditional\n",
    "      probabilities P(w3 | w1, w2), computed as count(w1, w2, w3) / count(w1, w2).\n",
    "  \"\"\"\n",
    "  trigram_counts = Counter(generate_ngrams(tokens, 3))\n",
    "  bigram_counts = Counter(generate_ngrams(tokens, 2))\n",
    "  return {\n",
    "      (w1, w2, w3): trigram_counts[(w1, w2, w3)] / bigram_counts[(w1, w2)]\n",
    "      for (w1, w2, w3) in trigram_counts\n",
    "  }\n",
    "\n",
    "def build_ngram_model(tokens, n):\n",
    "  \"\"\"\n",
    "  Dispatches to the appropriate n-gram model builder based on the value of n.\n",
    "\n",
    "  Args:\n",
    "      tokens (List[str]): A list of word tokens.\n",
    "      n (int): The n-gram size (1, 2, or 3).\n",
    "\n",
    "  Returns:\n",
    "      Dict: The n-gram model (unigram, bigram, or trigram).\n",
    "  \"\"\"\n",
    "  if n == 1:\n",
    "    return build_unigram_model(tokens)\n",
    "  elif n == 2:\n",
    "    return build_bigram_model(tokens)\n",
    "  elif n == 3:\n",
    "    return build_trigram_model(tokens)\n",
    "  else:\n",
    "    raise ValueError('Only n = 1, 2, or 3 are supported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_model = build_unigram_model(train_tokens)\n",
    "bigram_model = build_bigram_model(train_tokens)\n",
    "trigram_model = build_trigram_model(train_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eT3HO7Bd_ikJ"
   },
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unigram_text(model, length = 10):\n",
    "  \"\"\"\n",
    "  Generates text from a unigram model by sampling words based on their probabilities.\n",
    "\n",
    "  Args:\n",
    "      model (Dict[str, float]): A unigram probability model.\n",
    "      length (int): Number of words to generate. If None or 0, generation continues until </s> is seen.\n",
    "\n",
    "  Returns:\n",
    "      str: A generated sentence.\n",
    "  \"\"\"\n",
    "  sentence = []\n",
    "  if length:\n",
    "    for _ in range(length):\n",
    "      next_word = random.choices(list(model.keys()), weights = list(model.values()))[0]\n",
    "      sentence.append(next_word)\n",
    "  else:\n",
    "    while True:\n",
    "      next_word = random.choices(list(model.keys()), weights = list(model.values()))[0]\n",
    "      sentence.append(next_word)\n",
    "      if next_word == '</s>':\n",
    "        break\n",
    "  return ' '.join(sentence)\n",
    "\n",
    "def generate_bigram_text(model, seed_word, length = 10):\n",
    "  \"\"\"\n",
    "  Generates text from a bigram model using a single seed word.\n",
    "\n",
    "  Args:\n",
    "      model (Dict[Tuple[str, str], float]): A bigram probability model.\n",
    "      seed_word (str): The starting word of the sentence.\n",
    "      length (int): Total number of words to generate. If None or 0, continues until </s>.\n",
    "\n",
    "  Returns:\n",
    "      str: A generated sentence beginning with the seed word.\n",
    "  \"\"\"\n",
    "  sentence = [seed_word]\n",
    "  if length:\n",
    "    for _ in range(length - 1):\n",
    "      candidates = {k[1]:v for k, v in model.items() if k[0] == sentence[-1]}\n",
    "      if not candidates:\n",
    "        break\n",
    "      next_word = random.choices(list(candidates.keys()), weights = list(candidates.values()))[0]\n",
    "      sentence.append(next_word)\n",
    "  else:\n",
    "    while True:\n",
    "      candidates = {k[1]:v for k, v in model.items() if k[0] == sentence[-1]}\n",
    "      if not candidates:\n",
    "        break\n",
    "      next_word = random.choices(list(candidates.keys()), weights = list(candidates.values()))[0]\n",
    "      sentence.append(next_word)\n",
    "      if next_word == '</s>':\n",
    "        break\n",
    "  return ' '.join(sentence)\n",
    "\n",
    "def generate_trigram_text(model, seed_words, length = 10):\n",
    "  \"\"\"\n",
    "  Generates text from a trigram model using two seed words.\n",
    "\n",
    "  Args:\n",
    "      model (Dict[Tuple[str, str, str], float]): A trigram probability model.\n",
    "      seed_words (Tuple[str, str]): The starting two words.\n",
    "      length (int): Total number of words to generate. If None or 0, continues until </s>.\n",
    "\n",
    "  Returns:\n",
    "      str: A generated sentence beginning with the seed words.\n",
    "  \"\"\"\n",
    "  sentence = list(seed_words)\n",
    "  if length:\n",
    "    for _ in range(length - 2):\n",
    "      candidates = {k[2]:v for k, v in model.items() if k[:2] == tuple(sentence[-2:])}\n",
    "      if not candidates:\n",
    "        break\n",
    "      next_word = random.choices(list(candidates.keys()), weights = list(candidates.values()))[0]\n",
    "      sentence.append(next_word)\n",
    "  else:\n",
    "    while True:\n",
    "      candidates = {k[2]:v for k, v in model.items() if k[:2] == tuple(sentence[-2:])}\n",
    "      if not candidates:\n",
    "        break\n",
    "      next_word = random.choices(list(candidates.keys()), weights = list(candidates.values()))[0]\n",
    "      sentence.append(next_word)\n",
    "      if next_word == '</s>':\n",
    "        break\n",
    "  return ' '.join(sentence)\n",
    "\n",
    "def generate_ngram_text(model, seed_words, n, length = 10):\n",
    "  \"\"\"\n",
    "  Dispatch function to generate text from an n-gram model.\n",
    "\n",
    "  Args:\n",
    "      model (Dict): An n-gram model (unigram, bigram, or trigram).\n",
    "      seed_words (Union[str, Tuple[str, str]]): Seed word(s) used to start the sentence.\n",
    "      n (int): The type of model (1, 2, or 3).\n",
    "      length (int): Number of tokens to generate.\n",
    "\n",
    "  Returns:\n",
    "      None\n",
    "  \"\"\"\n",
    "  if n == 1:\n",
    "    print(generate_unigram_text(model, length))\n",
    "  elif n == 2:\n",
    "    print(generate_bigram_text(model, seed_words, length))\n",
    "  elif n == 3:\n",
    "    print(generate_trigram_text(model, seed_words, length))\n",
    "  else:\n",
    "    raise ValueError('Only n = 1, 2, or 3 are supported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Model:\n",
      "items to possible <s> N educational in products who </s> in corporation it sept. mr. which N a the in the among and <s> ads producer crime dispute a express <unk> that stop resources a <s> on mark plan with voters get communist to </s> future </s> of underwriters this\n",
      "\n",
      "Bigram Model:\n",
      "<s> under the soviet economy throughout the nation 's contract fell a set a bid but the department to gold has about N N N in the agreement were N points to age group </s> <s> the july </s> <s> as he later than N N to retire about $\n",
      "\n",
      "Trigram Model:\n",
      "<s> i 'm not certain </s> <s> garbage magazine billed as the first few minutes of trading and construction markets </s> <s> west german <unk> automatic citizens this year to N N this year people are getting a bargain hunt </s> <s> he even sold one a democrat and one\n"
     ]
    }
   ],
   "source": [
    "print('Unigram Model:')\n",
    "generate_ngram_text(unigram_model, None, 1, 50)\n",
    "print()\n",
    "print('Bigram Model:')\n",
    "generate_ngram_text(bigram_model, ('<s>'), 2, 50)\n",
    "print()\n",
    "print('Trigram Model:')\n",
    "generate_ngram_text(trigram_model, ('<s>', 'i'), 3, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trd8-zUTEJ2s"
   },
   "source": [
    "## Calculate Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, tokens, n):\n",
    "  '''\n",
    "  Calculates the perplexity of an n-gram model on a given token sequence.\n",
    "\n",
    "  Perplexity measures how well a probability model predicts a sequence.\n",
    "  Lower perplexity indicates better predictive performance.\n",
    "\n",
    "  Arguments:\n",
    "    - model: A dictionary representing the n-gram model (e.g., unigram, bigram, trigram)\n",
    "    - tokens: A list of tokens (words) from the text to evaluate\n",
    "    - n: The order of the n-gram (1 for unigram, 2 for bigram, etc.)\n",
    "\n",
    "  Returns:\n",
    "    - perplexity: A float value representing the model's perplexity on the token sequence\n",
    "\n",
    "  Notes:\n",
    "    - If an n-gram is not found in the model, a small probability (1e-6) is used.\n",
    "    - Uses base-2 logarithm for computing log probability.\n",
    "  '''\n",
    "  N = len(tokens)\n",
    "  log_prob_sum = 0\n",
    "\n",
    "  for t in range(len(tokens) - n + 1):\n",
    "    if n == 1:\n",
    "      ngram = tokens[t]\n",
    "    else:\n",
    "      ngram = tuple(tokens[t:t+n])\n",
    "    prob = model.get(ngram, 1e-6)\n",
    "    log_prob_sum += math.log2(prob)\n",
    "\n",
    "  return 2 ** (-log_prob_sum / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram test perplexity: 622.1353381284742\n",
      "Bigram test perplexity: 341.81849008051756\n",
      "Trigram test perplexity: 5355.641165309026\n"
     ]
    }
   ],
   "source": [
    "print('Unigram test perplexity:', calculate_perplexity(unigram_model, test_tokens, 1))\n",
    "print('Bigram test perplexity:', calculate_perplexity(bigram_model, test_tokens, 2))\n",
    "print('Trigram test perplexity:', calculate_perplexity(trigram_model, test_tokens, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TG_LImVNHUAS"
   },
   "source": [
    "## Fall Back Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fallback_perplexity(trigram_model, bigram_model, unigram_model, tokens):\n",
    "  '''\n",
    "  Calculates the perplexity of a fallback n-gram model on a given token sequence.\n",
    "\n",
    "  This function uses a backoff approach:\n",
    "  - It first tries to find the trigram probability\n",
    "  - If not found, it falls back to the bigram\n",
    "  - If the bigram is also missing, it falls back to the unigram\n",
    "  - If the unigram is missing, it uses a small default probability (1e-6)\n",
    "\n",
    "  Arguments:\n",
    "    - trigram_model: A dictionary of trigram probabilities\n",
    "    - bigram_model: A dictionary of bigram probabilities\n",
    "    - unigram_model: A dictionary of unigram probabilities\n",
    "    - tokens: A list of tokens to evaluate\n",
    "\n",
    "  Returns:\n",
    "    - perplexity: A float representing how well the fallback model predicts the token sequence\n",
    "\n",
    "  Notes:\n",
    "    - Uses log base 2\n",
    "    - The perplexity is normalized by the total number of tokens\n",
    "  '''\n",
    "  n = len(tokens)\n",
    "  log_prob_sum = 0\n",
    "  for i in range(2, n):\n",
    "    trigram = (tokens[i - 2], tokens[i - 1], tokens[i])\n",
    "    bigram = (tokens[i - 1], tokens[i])\n",
    "    unigram = tokens[i]\n",
    "    prob = trigram_model.get(trigram, bigram_model.get(bigram, unigram_model.get(unigram, 1e-6)))\n",
    "    log_prob_sum += math.log2(prob)\n",
    "\n",
    "  return 2 ** (-log_prob_sum / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fallback Perplexity: 77.84830636849189\n"
     ]
    }
   ],
   "source": [
    "print('Fallback Perplexity:', calculate_fallback_perplexity(trigram_model, bigram_model, unigram_model, test_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGM0ihaSJIli"
   },
   "source": [
    "## Laplace Smoothing - Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_model_leplace(tokens):\n",
    "  '''\n",
    "  Constructs a smoothed trigram language model using Laplace (add-one) smoothing.\n",
    "\n",
    "  Arguments:\n",
    "    - tokens: A list of tokens from a preprocessed corpus\n",
    "\n",
    "  Returns:\n",
    "    - smoothed_trigram_prob: A function that takes (w1, w2, w3) and returns the Laplace-smoothed trigram probability\n",
    "    - V: The vocabulary size (number of unique tokens)\n",
    "\n",
    "  How it works:\n",
    "    - Counts trigram and bigram frequencies from the token list\n",
    "    - Applies Laplace smoothing to avoid zero probabilities for unseen trigrams:\n",
    "        P(w3 | w1, w2) = (count(w1, w2, w3) + 1) / (count(w1, w2) + V)\n",
    "    - This helps the model assign a small probability to unseen sequences\n",
    "\n",
    "  Example:\n",
    "    prob_fn, vocab_size = trigram_model_leplace(tokens)\n",
    "    prob = prob_fn('the', 'cat', 'sat')\n",
    "  '''\n",
    "  trigrams = generate_ngrams(tokens, 3)\n",
    "  bigrams = generate_ngrams(tokens, 2)\n",
    "\n",
    "  trigram_counts = Counter(trigrams)\n",
    "  bigram_counts = Counter(bigrams)\n",
    "\n",
    "  V = len(set(tokens))\n",
    "\n",
    "  def smoothed_trigram_prob(w1, w2, w3):\n",
    "    trigram = (w1, w2, w3)\n",
    "    bigram = (w1, w2)\n",
    "\n",
    "    return (trigram_counts.get(trigram, 0) + 1) / (bigram_counts.get(bigram, 0) + V)\n",
    "\n",
    "  return smoothed_trigram_prob, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_trigram_prob, V = trigram_model_leplace(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trigram_smoothed_perplexity(smoothed_trigram_prob, test_tokens, V):\n",
    "  '''\n",
    "  Calculates the perplexity of a test sequence using a smoothed trigram model.\n",
    "\n",
    "  Arguments:\n",
    "    - smoothed_trigram_prob: A function that returns the probability of a trigram\n",
    "                              (e.g., from trigram_model_leplace)\n",
    "    - test_tokens: A list of tokens from the test corpus\n",
    "    - V: The vocabulary size used during smoothing (not directly used here, but kept for consistency)\n",
    "\n",
    "  Returns:\n",
    "    - Perplexity: A float value representing how well the model predicts the test set.\n",
    "                  Lower is better; higher implies the model is less confident.\n",
    "\n",
    "  Notes:\n",
    "    - Uses log base 2 for probability accumulation\n",
    "    - Starts from index 2 since trigrams require 2 previous tokens\n",
    "    - Applies the standard formula:\n",
    "        Perplexity = 2 ^ [ - (1/N) * ∑ log2 P(w_i | w_{i-2}, w_{i-1}) ]\n",
    "    - Assumes the input model already applies Laplace smoothing\n",
    "\n",
    "  Example:\n",
    "    prob_fn, V = trigram_model_leplace(train_tokens)\n",
    "    ppl = calculate_trigram_smoothed_perplexity(prob_fn, test_tokens, V)\n",
    "  '''\n",
    "  n = len(test_tokens)\n",
    "  log_prob_sum = 0\n",
    "  for i in range(2, n):\n",
    "    w1, w2, w3 = test_tokens[i - 2], test_tokens[i - 1], test_tokens[i]\n",
    "    prob = smoothed_trigram_prob(w1, w2, w3)\n",
    "    log_prob_sum += math.log2(prob)\n",
    "  return 2 ** (-log_prob_sum / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leplace smoothed trigram perplexity: 3116.601594194831\n"
     ]
    }
   ],
   "source": [
    "print('Leplace smoothed trigram perplexity:', calculate_trigram_smoothed_perplexity(smoothed_trigram_prob, test_tokens, V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmF01K7mNTQR"
   },
   "source": [
    "We see that the back-off method performs the best. This model's ability to use extended context when it is reliable and shorter context when it's not, makes it more dynamic than the other methods, including Laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
