2025-06-17 03:46:24,354 - [INFO] Tokenizer loaded: distilbert-base-uncased
2025-06-17 03:55:57,503 - [INFO] Dataset loaded and tokenized.
2025-06-17 03:55:57,504 - [INFO] Data loaders created.
2025-06-17 03:55:58,938 - [INFO] Model initialized.
2025-06-17 03:55:58,940 - [INFO] Optimizer initialized: AdamW with lr = 0.001
2025-06-17 03:55:58,940 - [INFO] Scheduler initialized. Mode = min, factor = 0.5 and patience = 2
2025-06-17 03:55:59,514 - [INFO] Loss function initialized.
2025-06-17 03:55:59,516 - [INFO] Epoch 1 / 10
2025-06-17 03:55:59,516 - [INFO] ------------------------------
2025-06-17 03:56:00,816 - [INFO] Batch 1 Loss: 0.6923
2025-06-17 03:56:50,786 - [INFO] Batch 101 Loss: 0.5087
2025-06-17 03:57:41,056 - [INFO] Batch 201 Loss: 0.4474
2025-06-17 03:58:31,341 - [INFO] Batch 301 Loss: 0.3946
2025-06-17 03:59:21,643 - [INFO] Batch 401 Loss: 0.4529
2025-06-17 04:00:11,990 - [INFO] Batch 501 Loss: 0.3887
2025-06-17 04:01:02,421 - [INFO] Batch 601 Loss: 0.4803
2025-06-17 04:01:52,788 - [INFO] Batch 701 Loss: 0.3863
2025-06-17 04:02:43,103 - [INFO] Batch 801 Loss: 0.4234
2025-06-17 04:03:33,452 - [INFO] Batch 901 Loss: 0.3797
2025-06-17 04:04:23,844 - [INFO] Batch 1001 Loss: 0.4394
2025-06-17 04:05:14,215 - [INFO] Batch 1101 Loss: 0.4441
2025-06-17 04:06:04,501 - [INFO] Batch 1201 Loss: 0.4185
2025-06-17 04:06:54,882 - [INFO] Batch 1301 Loss: 0.4229
2025-06-17 04:07:45,205 - [INFO] Batch 1401 Loss: 0.4640
2025-06-17 04:08:35,543 - [INFO] Batch 1501 Loss: 0.4495
2025-06-17 04:09:25,908 - [INFO] Batch 1601 Loss: 0.3705
2025-06-17 04:10:16,206 - [INFO] Batch 1701 Loss: 0.3883
2025-06-17 04:12:08,094 - [INFO] ================================================================================
2025-06-17 04:12:08,094 - [INFO] Train Loss: 0.4368 | Train AUPRC: 0.4652 | Val Loss: 0.4159 | Val AUPRC: 0.5130 | LR: 1.00e-03
2025-06-17 04:12:08,094 - [INFO] ================================================================================
2025-06-17 04:12:08,491 - [INFO] Checkpoint saved to ./models/distilbert_mean_pool_frozen_bert/checkpoint.pth
2025-06-17 04:12:08,491 - [INFO] 

2025-06-17 04:12:08,491 - [INFO] Epoch 2 / 10
2025-06-17 04:12:08,491 - [INFO] ------------------------------
2025-06-17 04:12:09,422 - [INFO] Batch 1 Loss: 0.3446
2025-06-17 04:12:59,718 - [INFO] Batch 101 Loss: 0.4757
2025-06-17 04:13:50,042 - [INFO] Batch 201 Loss: 0.3858
2025-06-17 04:14:40,393 - [INFO] Batch 301 Loss: 0.4246
2025-06-17 04:15:30,733 - [INFO] Batch 401 Loss: 0.3820
2025-06-17 04:16:21,057 - [INFO] Batch 501 Loss: 0.3757
2025-06-17 04:17:11,417 - [INFO] Batch 601 Loss: 0.4361
2025-06-17 04:18:01,814 - [INFO] Batch 701 Loss: 0.4429
2025-06-17 04:18:52,151 - [INFO] Batch 801 Loss: 0.5177
2025-06-17 04:19:42,477 - [INFO] Batch 901 Loss: 0.4211
2025-06-17 04:20:32,848 - [INFO] Batch 1001 Loss: 0.3841
2025-06-17 04:21:23,173 - [INFO] Batch 1101 Loss: 0.3838
2025-06-17 04:22:13,516 - [INFO] Batch 1201 Loss: 0.3542
2025-06-17 04:23:03,877 - [INFO] Batch 1301 Loss: 0.3734
2025-06-17 04:23:54,193 - [INFO] Batch 1401 Loss: 0.4178
2025-06-17 04:24:44,449 - [INFO] Batch 1501 Loss: 0.4604
2025-06-17 04:25:34,796 - [INFO] Batch 1601 Loss: 0.4101
2025-06-17 04:26:25,151 - [INFO] Batch 1701 Loss: 0.4010
2025-06-17 04:28:17,036 - [INFO] ================================================================================
2025-06-17 04:28:17,036 - [INFO] Train Loss: 0.4129 | Train AUPRC: 0.5051 | Val Loss: 0.3972 | Val AUPRC: 0.5303 | LR: 1.00e-03
2025-06-17 04:28:17,036 - [INFO] ================================================================================
2025-06-17 04:28:18,738 - [INFO] Checkpoint saved to ./models/distilbert_mean_pool_frozen_bert/checkpoint.pth
2025-06-17 04:28:18,738 - [INFO] 

2025-06-17 04:28:18,738 - [INFO] Epoch 3 / 10
2025-06-17 04:28:18,739 - [INFO] ------------------------------
2025-06-17 04:28:19,705 - [INFO] Batch 1 Loss: 0.4147
2025-06-17 04:29:10,020 - [INFO] Batch 101 Loss: 0.4688
2025-06-17 04:30:00,341 - [INFO] Batch 201 Loss: 0.4217
2025-06-17 04:30:50,724 - [INFO] Batch 301 Loss: 0.4230
2025-06-17 04:31:41,158 - [INFO] Batch 401 Loss: 0.4060
2025-06-17 04:32:31,465 - [INFO] Batch 501 Loss: 0.4538
2025-06-17 04:33:21,851 - [INFO] Batch 601 Loss: 0.4106
2025-06-17 04:34:12,223 - [INFO] Batch 701 Loss: 0.3942
2025-06-17 04:35:02,576 - [INFO] Batch 801 Loss: 0.4771
2025-06-17 04:35:52,890 - [INFO] Batch 901 Loss: 0.4245
2025-06-17 04:36:43,251 - [INFO] Batch 1001 Loss: 0.4850
2025-06-17 04:37:33,594 - [INFO] Batch 1101 Loss: 0.4360
2025-06-17 04:38:24,029 - [INFO] Batch 1201 Loss: 0.3916
2025-06-17 04:39:14,448 - [INFO] Batch 1301 Loss: 0.3929
2025-06-17 04:40:04,859 - [INFO] Batch 1401 Loss: 0.4432
2025-06-17 04:40:55,227 - [INFO] Batch 1501 Loss: 0.3669
2025-06-17 04:41:45,646 - [INFO] Batch 1601 Loss: 0.4192
2025-06-17 04:42:35,992 - [INFO] Batch 1701 Loss: 0.4319
2025-06-17 04:44:28,093 - [INFO] ================================================================================
2025-06-17 04:44:28,093 - [INFO] Train Loss: 0.4051 | Train AUPRC: 0.5184 | Val Loss: 0.3907 | Val AUPRC: 0.5428 | LR: 1.00e-03
2025-06-17 04:44:28,093 - [INFO] ================================================================================
2025-06-17 04:44:29,857 - [INFO] Checkpoint saved to ./models/distilbert_mean_pool_frozen_bert/checkpoint.pth
2025-06-17 04:44:29,857 - [INFO] 

2025-06-17 04:44:29,857 - [INFO] Epoch 4 / 10
2025-06-17 04:44:29,857 - [INFO] ------------------------------
2025-06-17 04:44:30,788 - [INFO] Batch 1 Loss: 0.4525
2025-06-17 04:45:21,153 - [INFO] Batch 101 Loss: 0.3676
2025-06-17 04:46:11,572 - [INFO] Batch 201 Loss: 0.3756
2025-06-17 04:47:02,019 - [INFO] Batch 301 Loss: 0.3984
2025-06-17 04:47:52,420 - [INFO] Batch 401 Loss: 0.3236
2025-06-17 04:48:42,797 - [INFO] Batch 501 Loss: 0.3939
2025-06-17 04:49:33,107 - [INFO] Batch 601 Loss: 0.3473
2025-06-17 04:50:23,468 - [INFO] Batch 701 Loss: 0.3883
2025-06-17 19:17:52,622 - [INFO] Tokenizer loaded: distilbert-base-uncased
2025-06-17 19:27:18,922 - [INFO] Dataset loaded and tokenized.
2025-06-17 19:27:18,922 - [INFO] Data loaders created.
2025-06-17 19:27:22,453 - [INFO] Model initialized.
2025-06-17 19:27:22,454 - [INFO] Optimizer initialized: AdamW with lr = 0.001
2025-06-17 19:27:22,454 - [INFO] Scheduler initialized. Mode = min, factor = 0.5 and patience = 2
2025-06-17 19:27:23,032 - [INFO] Loss function initialized.
2025-06-17 19:27:24,444 - [INFO] Loaded checkpoint from ./models/distilbert_mean_pool_frozen_bert/checkpoint.pth
2025-06-17 19:27:24,445 - [INFO] Resuming training from epoch 4 with best_val_auprc = 0.5428
2025-06-17 19:27:24,446 - [INFO] Epoch 4 / 10
2025-06-17 19:27:24,446 - [INFO] ------------------------------
2025-06-17 19:27:26,051 - [INFO] Batch 1 Loss: 0.3650
2025-06-17 19:28:16,435 - [INFO] Batch 101 Loss: 0.4290
2025-06-17 19:29:06,867 - [INFO] Batch 201 Loss: 0.3748
2025-06-17 19:29:57,308 - [INFO] Batch 301 Loss: 0.3545
2025-06-17 19:30:47,757 - [INFO] Batch 401 Loss: 0.3857
2025-06-17 19:31:38,125 - [INFO] Batch 501 Loss: 0.3467
2025-06-17 19:32:28,545 - [INFO] Batch 601 Loss: 0.4411
2025-06-17 19:33:18,971 - [INFO] Batch 701 Loss: 0.3556
2025-06-17 19:34:09,410 - [INFO] Batch 801 Loss: 0.3977
2025-06-17 19:34:59,855 - [INFO] Batch 901 Loss: 0.3525
2025-06-17 19:35:50,287 - [INFO] Batch 1001 Loss: 0.4032
2025-06-17 19:36:40,732 - [INFO] Batch 1101 Loss: 0.3965
2025-06-17 19:37:31,174 - [INFO] Batch 1201 Loss: 0.4187
2025-06-17 19:38:21,554 - [INFO] Batch 1301 Loss: 0.4123
2025-06-17 19:39:11,989 - [INFO] Batch 1401 Loss: 0.4324
2025-06-17 19:40:02,458 - [INFO] Batch 1501 Loss: 0.4277
2025-06-17 19:40:52,925 - [INFO] Batch 1601 Loss: 0.3554
2025-06-17 19:41:43,286 - [INFO] Batch 1701 Loss: 0.3432
2025-06-17 19:43:35,343 - [INFO] ================================================================================
2025-06-17 19:43:35,343 - [INFO] Train Loss: 0.3994 | Train AUPRC: 0.5267 | Val Loss: 0.3934 | Val AUPRC: 0.5448 | LR: 1.00e-03
2025-06-17 19:43:35,343 - [INFO] ================================================================================
2025-06-17 19:43:37,054 - [INFO] Checkpoint saved to ./models/distilbert_mean_pool_frozen_bert/checkpoint.pth
2025-06-17 19:43:37,054 - [INFO] 

2025-06-17 19:43:37,054 - [INFO] Epoch 5 / 10
2025-06-17 19:43:37,054 - [INFO] ------------------------------
2025-06-17 19:43:37,976 - [INFO] Batch 1 Loss: 0.3171
2025-06-17 19:44:28,564 - [INFO] Batch 101 Loss: 0.4714
2025-06-17 19:45:19,029 - [INFO] Batch 201 Loss: 0.3694
2025-06-17 19:46:09,395 - [INFO] Batch 301 Loss: 0.3933
2025-06-17 19:46:59,813 - [INFO] Batch 401 Loss: 0.3703
2025-06-17 19:47:50,249 - [INFO] Batch 501 Loss: 0.3455
2025-06-17 19:48:40,715 - [INFO] Batch 601 Loss: 0.4210
2025-06-17 19:49:31,080 - [INFO] Batch 701 Loss: 0.4335
2025-06-17 19:50:21,519 - [INFO] Batch 801 Loss: 0.5048
2025-06-17 19:51:12,036 - [INFO] Batch 901 Loss: 0.4063
2025-06-17 19:52:02,472 - [INFO] Batch 1001 Loss: 0.3766
2025-06-17 19:52:52,847 - [INFO] Batch 1101 Loss: 0.3902
2025-06-17 19:53:43,272 - [INFO] Batch 1201 Loss: 0.3443
2025-06-17 19:54:33,723 - [INFO] Batch 1301 Loss: 0.3625
2025-06-17 19:55:24,172 - [INFO] Batch 1401 Loss: 0.4075
2025-06-17 19:56:14,540 - [INFO] Batch 1501 Loss: 0.4291
2025-06-17 19:57:04,980 - [INFO] Batch 1601 Loss: 0.3915
2025-06-17 19:57:55,420 - [INFO] Batch 1701 Loss: 0.3815
2025-06-17 19:59:47,450 - [INFO] ================================================================================
2025-06-17 19:59:47,450 - [INFO] Train Loss: 0.3952 | Train AUPRC: 0.5320 | Val Loss: 0.3858 | Val AUPRC: 0.5513 | LR: 1.00e-03
2025-06-17 19:59:47,450 - [INFO] ================================================================================
2025-06-17 19:59:49,149 - [INFO] Checkpoint saved to ./models/distilbert_mean_pool_frozen_bert/checkpoint.pth
2025-06-17 19:59:49,149 - [INFO] 

2025-06-17 19:59:49,149 - [INFO] Epoch 6 / 10
2025-06-17 19:59:49,149 - [INFO] ------------------------------
2025-06-17 19:59:50,082 - [INFO] Batch 1 Loss: 0.3961
2025-06-17 20:00:40,517 - [INFO] Batch 101 Loss: 0.4583
2025-06-17 20:01:30,946 - [INFO] Batch 201 Loss: 0.3995
2025-06-17 20:02:21,394 - [INFO] Batch 301 Loss: 0.4071
2025-06-17 20:03:11,869 - [INFO] Batch 401 Loss: 0.3870
2025-06-17 20:04:02,243 - [INFO] Batch 501 Loss: 0.4393
2025-06-17 20:04:52,674 - [INFO] Batch 601 Loss: 0.4067
2025-06-17 20:05:43,118 - [INFO] Batch 701 Loss: 0.3694
2025-06-17 20:06:33,546 - [INFO] Batch 801 Loss: 0.4639
2025-06-17 20:07:23,921 - [INFO] Batch 901 Loss: 0.4268
2025-06-17 20:08:14,367 - [INFO] Batch 1001 Loss: 0.4615
2025-06-17 20:09:04,803 - [INFO] Batch 1101 Loss: 0.4288
2025-06-17 20:09:55,248 - [INFO] Batch 1201 Loss: 0.3917
2025-06-17 20:10:45,625 - [INFO] Batch 1301 Loss: 0.3862
2025-06-17 20:11:36,063 - [INFO] Batch 1401 Loss: 0.4422
2025-06-17 20:12:26,501 - [INFO] Batch 1501 Loss: 0.3469
2025-06-17 20:13:16,933 - [INFO] Batch 1601 Loss: 0.3891
2025-06-17 20:14:07,320 - [INFO] Batch 1701 Loss: 0.4275
2025-06-17 20:15:59,505 - [INFO] ================================================================================
2025-06-17 20:15:59,505 - [INFO] Train Loss: 0.3928 | Train AUPRC: 0.5354 | Val Loss: 0.3816 | Val AUPRC: 0.5568 | LR: 1.00e-03
2025-06-17 20:15:59,505 - [INFO] ================================================================================
2025-06-17 20:16:01,210 - [INFO] Checkpoint saved to ./models/distilbert_mean_pool_frozen_bert/checkpoint.pth
2025-06-17 20:16:01,211 - [INFO] 

2025-06-17 20:16:01,211 - [INFO] Epoch 7 / 10
2025-06-17 20:16:01,211 - [INFO] ------------------------------
2025-06-17 20:16:02,141 - [INFO] Batch 1 Loss: 0.4323
2025-06-17 20:16:52,604 - [INFO] Batch 101 Loss: 0.3551
2025-06-17 20:17:43,033 - [INFO] Batch 201 Loss: 0.3632
2025-06-17 20:18:33,400 - [INFO] Batch 301 Loss: 0.3901
2025-06-17 20:19:23,823 - [INFO] Batch 401 Loss: 0.3232
2025-06-17 20:20:14,266 - [INFO] Batch 501 Loss: 0.3776
2025-06-17 20:21:04,733 - [INFO] Batch 601 Loss: 0.3367
2025-06-17 20:21:55,099 - [INFO] Batch 701 Loss: 0.3839
2025-06-17 20:22:45,532 - [INFO] Batch 801 Loss: 0.4012
2025-06-17 20:23:35,965 - [INFO] Batch 901 Loss: 0.3270
2025-06-17 20:24:26,409 - [INFO] Batch 1001 Loss: 0.4179
2025-06-17 20:25:16,765 - [INFO] Batch 1101 Loss: 0.3662
2025-06-17 20:26:07,214 - [INFO] Batch 1201 Loss: 0.4024
2025-06-17 20:26:57,656 - [INFO] Batch 1301 Loss: 0.3550
2025-06-17 20:27:48,079 - [INFO] Batch 1401 Loss: 0.3809
2025-06-17 20:28:38,447 - [INFO] Batch 1501 Loss: 0.4409
2025-06-17 20:29:28,883 - [INFO] Batch 1601 Loss: 0.3591
2025-06-17 20:30:19,379 - [INFO] Batch 1701 Loss: 0.3861
2025-06-17 20:32:11,524 - [INFO] ================================================================================
2025-06-17 20:32:11,525 - [INFO] Train Loss: 0.3916 | Train AUPRC: 0.5379 | Val Loss: 0.3835 | Val AUPRC: 0.5574 | LR: 1.00e-03
2025-06-17 20:32:11,525 - [INFO] ================================================================================
2025-06-17 20:32:13,247 - [INFO] Checkpoint saved to ./models/distilbert_mean_pool_frozen_bert/checkpoint.pth
2025-06-17 20:32:13,247 - [INFO] 

2025-06-17 20:32:13,248 - [INFO] Epoch 8 / 10
2025-06-17 20:32:13,248 - [INFO] ------------------------------
2025-06-17 20:32:14,201 - [INFO] Batch 1 Loss: 0.3847
2025-06-17 20:33:04,647 - [INFO] Batch 101 Loss: 0.4096
2025-06-17 20:33:55,094 - [INFO] Batch 201 Loss: 0.3819
2025-06-17 20:34:45,533 - [INFO] Batch 301 Loss: 0.4136
2025-06-17 20:35:35,974 - [INFO] Batch 401 Loss: 0.4187
2025-06-17 20:36:26,353 - [INFO] Batch 501 Loss: 0.3491
2025-06-17 20:37:16,797 - [INFO] Batch 601 Loss: 0.4005
2025-06-17 20:38:07,250 - [INFO] Batch 701 Loss: 0.3648
2025-06-17 20:38:57,671 - [INFO] Batch 801 Loss: 0.3938
2025-06-17 20:39:48,058 - [INFO] Batch 901 Loss: 0.4062
2025-06-17 20:40:38,487 - [INFO] Batch 1001 Loss: 0.4446
2025-06-17 20:41:28,933 - [INFO] Batch 1101 Loss: 0.4263
2025-06-17 20:42:19,367 - [INFO] Batch 1201 Loss: 0.3980
2025-06-17 20:43:09,738 - [INFO] Batch 1301 Loss: 0.3987
2025-06-17 20:44:00,184 - [INFO] Batch 1401 Loss: 0.4118
2025-06-17 20:44:50,649 - [INFO] Batch 1501 Loss: 0.3967
2025-06-17 20:45:41,090 - [INFO] Batch 1601 Loss: 0.4030
2025-06-17 20:46:31,475 - [INFO] Batch 1701 Loss: 0.4436
2025-06-17 20:48:23,657 - [INFO] ================================================================================
2025-06-17 20:48:23,657 - [INFO] Train Loss: 0.3892 | Train AUPRC: 0.5415 | Val Loss: 0.3866 | Val AUPRC: 0.5533 | LR: 1.00e-03
2025-06-17 20:48:23,658 - [INFO] ================================================================================
2025-06-17 20:48:23,658 - [INFO] No improvement in validation AUPRC for 1 epochs.
2025-06-17 20:48:23,658 - [INFO] 

2025-06-17 20:48:23,658 - [INFO] Epoch 9 / 10
2025-06-17 20:48:23,658 - [INFO] ------------------------------
2025-06-17 20:48:24,605 - [INFO] Batch 1 Loss: 0.4355
2025-06-17 20:49:15,076 - [INFO] Batch 101 Loss: 0.3798
2025-06-17 20:50:05,486 - [INFO] Batch 201 Loss: 0.3797
2025-06-17 20:50:55,871 - [INFO] Batch 301 Loss: 0.3731
2025-06-17 20:51:46,333 - [INFO] Batch 401 Loss: 0.3833
2025-06-17 20:52:36,784 - [INFO] Batch 501 Loss: 0.4373
2025-06-17 20:53:27,181 - [INFO] Batch 601 Loss: 0.3414
2025-06-17 20:54:17,577 - [INFO] Batch 701 Loss: 0.4249
2025-06-17 20:55:08,014 - [INFO] Batch 801 Loss: 0.4145
2025-06-17 20:55:58,454 - [INFO] Batch 901 Loss: 0.3601
2025-06-17 20:56:48,856 - [INFO] Batch 1001 Loss: 0.3707
2025-06-17 20:57:39,265 - [INFO] Batch 1101 Loss: 0.3682
2025-06-17 20:58:29,700 - [INFO] Batch 1201 Loss: 0.3816
2025-06-17 20:59:20,142 - [INFO] Batch 1301 Loss: 0.3713
2025-06-17 21:00:10,548 - [INFO] Batch 1401 Loss: 0.3931
2025-06-17 21:01:00,939 - [INFO] Batch 1501 Loss: 0.4084
2025-06-17 21:01:51,382 - [INFO] Batch 1601 Loss: 0.3687
2025-06-17 21:02:41,809 - [INFO] Batch 1701 Loss: 0.3870
2025-06-17 21:04:33,891 - [INFO] ================================================================================
2025-06-17 21:04:33,891 - [INFO] Train Loss: 0.3874 | Train AUPRC: 0.5432 | Val Loss: 0.3894 | Val AUPRC: 0.5579 | LR: 5.00e-04
2025-06-17 21:04:33,891 - [INFO] ================================================================================
2025-06-17 21:04:35,699 - [INFO] Checkpoint saved to ./models/distilbert_mean_pool_frozen_bert/checkpoint.pth
2025-06-17 21:04:35,699 - [INFO] 

2025-06-17 21:04:35,699 - [INFO] Epoch 10 / 10
2025-06-17 21:04:35,699 - [INFO] ------------------------------
2025-06-17 21:04:36,703 - [INFO] Batch 1 Loss: 0.3656
2025-06-17 21:05:27,148 - [INFO] Batch 101 Loss: 0.3576
2025-06-17 21:06:17,598 - [INFO] Batch 201 Loss: 0.3268
2025-06-17 21:07:08,042 - [INFO] Batch 301 Loss: 0.4175
2025-06-17 21:07:58,446 - [INFO] Batch 401 Loss: 0.3441
2025-06-17 21:08:48,860 - [INFO] Batch 501 Loss: 0.4046
2025-06-17 21:09:39,305 - [INFO] Batch 601 Loss: 0.3860
2025-06-17 21:10:29,755 - [INFO] Batch 701 Loss: 0.3287
2025-06-17 21:11:20,143 - [INFO] Batch 801 Loss: 0.3415
2025-06-17 21:12:10,572 - [INFO] Batch 901 Loss: 0.3259
2025-06-17 21:13:01,011 - [INFO] Batch 1001 Loss: 0.3530
2025-06-17 21:13:51,436 - [INFO] Batch 1101 Loss: 0.3689
2025-06-17 21:14:41,843 - [INFO] Batch 1201 Loss: 0.3868
2025-06-17 21:15:32,250 - [INFO] Batch 1301 Loss: 0.3593
2025-06-17 21:16:22,692 - [INFO] Batch 1401 Loss: 0.3568
2025-06-17 21:17:13,162 - [INFO] Batch 1501 Loss: 0.3995
2025-06-17 21:18:03,570 - [INFO] Batch 1601 Loss: 0.3410
2025-06-17 21:18:53,970 - [INFO] Batch 1701 Loss: 0.4254
2025-06-17 21:20:46,093 - [INFO] ================================================================================
2025-06-17 21:20:46,093 - [INFO] Train Loss: 0.3807 | Train AUPRC: 0.5527 | Val Loss: 0.3779 | Val AUPRC: 0.5642 | LR: 5.00e-04
2025-06-17 21:20:46,093 - [INFO] ================================================================================
2025-06-17 21:20:47,835 - [INFO] Checkpoint saved to ./models/distilbert_mean_pool_frozen_bert/checkpoint.pth
2025-06-17 21:20:47,835 - [INFO] 

2025-06-17 21:20:47,863 - [INFO] Training complete!
2025-06-17 21:20:48,252 - [INFO] Model and training history saved successfully.
2025-06-17 21:20:48,561 - [INFO] Loss plot saved!
2025-06-17 21:20:48,562 - [INFO] Evaluating on threshold tuning set...
2025-06-17 21:22:08,105 - [INFO] Finding the best threshold...
2025-06-17 21:22:10,658 - [INFO] Best threshold: 0.8200 | Best score: 0.5366
2025-06-17 21:22:10,659 - [INFO] Evaluating on test set...
2025-06-17 21:24:49,570 - [INFO] Saving test predictions and evaluation plots...
2025-06-17 21:24:50,856 - [INFO] Test predictions and evaluation plots saved!
2025-06-17 21:24:50,857 - [INFO] Training and evaluation complete!
