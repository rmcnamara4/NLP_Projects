2025-06-16 02:36:25,680 - [INFO] Tokenizer loaded: distilbert-base-uncased
2025-06-16 02:40:04,474 - [INFO] Tokenizer loaded: distilbert-base-uncased
2025-06-16 02:47:23,189 - [INFO] Tokenizer loaded: distilbert-base-uncased
2025-06-16 02:56:59,289 - [INFO] Dataset loaded and tokenized.
2025-06-16 02:56:59,290 - [INFO] Data loaders created.
2025-06-16 02:57:00,762 - [INFO] Model initialized.
2025-06-16 02:57:00,764 - [INFO] Optimizer initialized: AdamW with lr = 0.001
2025-06-16 02:57:00,764 - [INFO] Scheduler initialized. Mode = min, factor = 0.5 and patience = 2
2025-06-16 02:57:01,340 - [INFO] Loss function initialized.
2025-06-16 02:57:01,342 - [INFO] Epoch 1 / 10
2025-06-16 02:57:01,342 - [INFO] ------------------------------
2025-06-16 02:57:02,741 - [INFO] Batch 1 Loss: 0.6923
2025-06-16 02:57:53,331 - [INFO] Batch 101 Loss: 0.5394
2025-06-16 02:58:44,049 - [INFO] Batch 201 Loss: 0.4666
2025-06-16 02:59:34,962 - [INFO] Batch 301 Loss: 0.4336
2025-06-16 03:00:25,879 - [INFO] Batch 401 Loss: 0.4591
2025-06-16 03:01:16,765 - [INFO] Batch 501 Loss: 0.4273
2025-06-16 03:02:07,670 - [INFO] Batch 601 Loss: 0.4814
2025-06-16 03:02:58,578 - [INFO] Batch 701 Loss: 0.4188
2025-06-16 03:03:49,468 - [INFO] Batch 801 Loss: 0.4295
2025-06-16 03:04:40,425 - [INFO] Batch 901 Loss: 0.3946
2025-06-16 03:05:31,378 - [INFO] Batch 1001 Loss: 0.4694
2025-06-16 03:06:22,243 - [INFO] Batch 1101 Loss: 0.4950
2025-06-16 03:07:13,142 - [INFO] Batch 1201 Loss: 0.4410
2025-06-16 03:08:03,956 - [INFO] Batch 1301 Loss: 0.4583
2025-06-16 03:08:54,790 - [INFO] Batch 1401 Loss: 0.4831
2025-06-16 03:09:45,724 - [INFO] Batch 1501 Loss: 0.4677
2025-06-16 03:10:36,604 - [INFO] Batch 1601 Loss: 0.4312
2025-06-16 03:11:27,427 - [INFO] Batch 1701 Loss: 0.4773
2025-06-16 03:13:20,904 - [INFO] ================================================================================
2025-06-16 03:13:20,905 - [INFO] Train Loss: 0.4693 | Train AUPRC: 0.4069 | Val Loss: 0.4343 | Val AUPRC: 0.4667 | LR: 1.00e-03
2025-06-16 03:13:20,905 - [INFO] ================================================================================
2025-06-16 03:13:21,296 - [INFO] Checkpoint saved to ./models/distilbert_cls_frozen_bert/checkpoint.pth
2025-06-16 03:13:21,296 - [INFO] 

2025-06-16 03:13:21,296 - [INFO] Epoch 2 / 10
2025-06-16 03:13:21,296 - [INFO] ------------------------------
2025-06-16 03:13:22,240 - [INFO] Batch 1 Loss: 0.3796
2025-06-16 03:14:12,990 - [INFO] Batch 101 Loss: 0.5154
2025-06-16 03:15:03,846 - [INFO] Batch 201 Loss: 0.3979
2025-06-16 03:15:54,638 - [INFO] Batch 301 Loss: 0.4522
2025-06-16 03:16:45,442 - [INFO] Batch 401 Loss: 0.3970
2025-06-16 03:17:36,332 - [INFO] Batch 501 Loss: 0.4284
2025-06-16 03:18:27,262 - [INFO] Batch 601 Loss: 0.4768
2025-06-16 03:19:18,193 - [INFO] Batch 701 Loss: 0.4822
2025-06-16 03:20:09,123 - [INFO] Batch 801 Loss: 0.4999
2025-06-16 03:21:00,000 - [INFO] Batch 901 Loss: 0.4807
2025-06-16 03:21:50,938 - [INFO] Batch 1001 Loss: 0.4355
2025-06-16 03:22:41,864 - [INFO] Batch 1101 Loss: 0.4370
2025-06-16 03:23:32,691 - [INFO] Batch 1201 Loss: 0.4078
2025-06-16 03:24:23,629 - [INFO] Batch 1301 Loss: 0.3874
2025-06-16 03:25:14,516 - [INFO] Batch 1401 Loss: 0.4653
2025-06-16 03:26:05,442 - [INFO] Batch 1501 Loss: 0.5585
2025-06-16 03:26:56,280 - [INFO] Batch 1601 Loss: 0.4786
2025-06-16 03:27:47,218 - [INFO] Batch 1701 Loss: 0.4460
2025-06-16 03:29:40,936 - [INFO] ================================================================================
2025-06-16 03:29:40,936 - [INFO] Train Loss: 0.4500 | Train AUPRC: 0.4403 | Val Loss: 0.4271 | Val AUPRC: 0.4824 | LR: 1.00e-03
2025-06-16 03:29:40,936 - [INFO] ================================================================================
2025-06-16 03:29:42,644 - [INFO] Checkpoint saved to ./models/distilbert_cls_frozen_bert/checkpoint.pth
2025-06-16 03:29:42,645 - [INFO] 

2025-06-16 03:29:42,645 - [INFO] Epoch 3 / 10
2025-06-16 03:29:42,645 - [INFO] ------------------------------
2025-06-16 03:29:43,619 - [INFO] Batch 1 Loss: 0.4291
2025-06-16 03:30:34,257 - [INFO] Batch 101 Loss: 0.4910
2025-06-16 03:31:24,995 - [INFO] Batch 201 Loss: 0.4609
2025-06-16 03:32:15,821 - [INFO] Batch 301 Loss: 0.4611
2025-06-16 03:33:06,657 - [INFO] Batch 401 Loss: 0.3967
2025-06-16 03:33:57,565 - [INFO] Batch 501 Loss: 0.4748
2025-06-16 03:34:48,394 - [INFO] Batch 601 Loss: 0.4169
2025-06-16 03:35:39,221 - [INFO] Batch 701 Loss: 0.4524
2025-06-16 03:36:30,077 - [INFO] Batch 801 Loss: 0.5035
2025-06-16 03:37:20,998 - [INFO] Batch 901 Loss: 0.4468
2025-06-16 03:38:11,894 - [INFO] Batch 1001 Loss: 0.4868
2025-06-16 03:39:02,819 - [INFO] Batch 1101 Loss: 0.4643
2025-06-16 03:39:53,692 - [INFO] Batch 1201 Loss: 0.4392
2025-06-16 03:40:44,580 - [INFO] Batch 1301 Loss: 0.4163
2025-06-16 03:41:35,508 - [INFO] Batch 1401 Loss: 0.4610
2025-06-16 03:42:26,212 - [INFO] Batch 1501 Loss: 0.4234
2025-06-16 03:43:17,145 - [INFO] Batch 1601 Loss: 0.4622
2025-06-16 03:44:08,080 - [INFO] Batch 1701 Loss: 0.4444
2025-06-16 03:46:01,636 - [INFO] ================================================================================
2025-06-16 03:46:01,636 - [INFO] Train Loss: 0.4441 | Train AUPRC: 0.4501 | Val Loss: 0.4202 | Val AUPRC: 0.4898 | LR: 1.00e-03
2025-06-16 03:46:01,636 - [INFO] ================================================================================
2025-06-16 03:46:03,362 - [INFO] Checkpoint saved to ./models/distilbert_cls_frozen_bert/checkpoint.pth
2025-06-16 03:46:03,362 - [INFO] 

2025-06-16 03:46:03,363 - [INFO] Epoch 4 / 10
2025-06-16 03:46:03,363 - [INFO] ------------------------------
2025-06-16 03:46:04,506 - [INFO] Batch 1 Loss: 0.4933
2025-06-16 03:46:55,198 - [INFO] Batch 101 Loss: 0.4445
2025-06-16 03:47:45,991 - [INFO] Batch 201 Loss: 0.4139
2025-06-16 03:48:36,784 - [INFO] Batch 301 Loss: 0.4498
2025-06-16 03:49:27,702 - [INFO] Batch 401 Loss: 0.3772
2025-06-16 03:50:18,638 - [INFO] Batch 501 Loss: 0.4653
2025-06-16 03:51:09,572 - [INFO] Batch 601 Loss: 0.4017
2025-06-16 03:52:00,357 - [INFO] Batch 701 Loss: 0.4654
2025-06-16 03:52:51,266 - [INFO] Batch 801 Loss: 0.4843
2025-06-16 03:53:42,188 - [INFO] Batch 901 Loss: 0.3932
2025-06-16 03:54:33,092 - [INFO] Batch 1001 Loss: 0.4746
2025-06-16 03:55:24,007 - [INFO] Batch 1101 Loss: 0.4190
2025-06-16 03:56:14,846 - [INFO] Batch 1201 Loss: 0.4276
2025-06-16 03:57:05,774 - [INFO] Batch 1301 Loss: 0.3826
2025-06-16 03:57:56,684 - [INFO] Batch 1401 Loss: 0.4251
2025-06-16 03:58:47,585 - [INFO] Batch 1501 Loss: 0.4548
2025-06-16 03:59:38,407 - [INFO] Batch 1601 Loss: 0.4002
2025-06-16 04:00:29,221 - [INFO] Batch 1701 Loss: 0.4169
2025-06-16 04:02:22,873 - [INFO] ================================================================================
2025-06-16 04:02:22,873 - [INFO] Train Loss: 0.4401 | Train AUPRC: 0.4559 | Val Loss: 0.4184 | Val AUPRC: 0.4946 | LR: 1.00e-03
2025-06-16 04:02:22,873 - [INFO] ================================================================================
2025-06-16 04:02:24,588 - [INFO] Checkpoint saved to ./models/distilbert_cls_frozen_bert/checkpoint.pth
2025-06-16 04:02:24,588 - [INFO] 

2025-06-16 04:02:24,588 - [INFO] Epoch 5 / 10
2025-06-16 04:02:24,588 - [INFO] ------------------------------
2025-06-16 04:02:25,566 - [INFO] Batch 1 Loss: 0.3863
2025-06-16 04:03:16,323 - [INFO] Batch 101 Loss: 0.4609
2025-06-16 04:04:07,065 - [INFO] Batch 201 Loss: 0.4342
2025-06-16 04:04:57,752 - [INFO] Batch 301 Loss: 0.4997
2025-06-16 04:05:48,593 - [INFO] Batch 401 Loss: 0.4633
2025-06-16 04:06:39,444 - [INFO] Batch 501 Loss: 0.4270
2025-06-16 04:07:30,360 - [INFO] Batch 601 Loss: 0.4028
2025-06-16 04:08:21,245 - [INFO] Batch 701 Loss: 0.4112
2025-06-16 04:09:12,095 - [INFO] Batch 801 Loss: 0.4587
2025-06-16 04:10:02,982 - [INFO] Batch 901 Loss: 0.4703
2025-06-16 04:10:53,911 - [INFO] Batch 1001 Loss: 0.4893
2025-06-16 04:11:44,806 - [INFO] Batch 1101 Loss: 0.4569
2025-06-16 04:12:35,693 - [INFO] Batch 1201 Loss: 0.4662
2025-06-16 04:13:26,651 - [INFO] Batch 1301 Loss: 0.4315
2025-06-16 04:14:17,567 - [INFO] Batch 1401 Loss: 0.4299
2025-06-16 04:15:08,443 - [INFO] Batch 1501 Loss: 0.4451
2025-06-16 04:15:59,377 - [INFO] Batch 1601 Loss: 0.4318
2025-06-16 04:16:50,240 - [INFO] Batch 1701 Loss: 0.4632
2025-06-16 04:18:43,905 - [INFO] ================================================================================
2025-06-16 04:18:43,905 - [INFO] Train Loss: 0.4372 | Train AUPRC: 0.4610 | Val Loss: 0.4211 | Val AUPRC: 0.4969 | LR: 1.00e-03
2025-06-16 04:18:43,906 - [INFO] ================================================================================
2025-06-16 04:18:45,652 - [INFO] Checkpoint saved to ./models/distilbert_cls_frozen_bert/checkpoint.pth
2025-06-16 04:18:45,652 - [INFO] 

2025-06-16 04:18:45,652 - [INFO] Epoch 6 / 10
2025-06-16 04:18:45,653 - [INFO] ------------------------------
2025-06-16 04:18:46,638 - [INFO] Batch 1 Loss: 0.4653
2025-06-16 04:19:37,382 - [INFO] Batch 101 Loss: 0.4340
2025-06-16 04:20:28,176 - [INFO] Batch 201 Loss: 0.4361
2025-06-16 04:21:18,985 - [INFO] Batch 301 Loss: 0.4337
2025-06-16 04:22:09,750 - [INFO] Batch 401 Loss: 0.4331
2025-06-16 04:23:00,668 - [INFO] Batch 501 Loss: 0.4483
2025-06-16 04:23:51,608 - [INFO] Batch 601 Loss: 0.4152
2025-06-16 04:24:42,537 - [INFO] Batch 701 Loss: 0.4716
2025-06-16 04:25:33,444 - [INFO] Batch 801 Loss: 0.4725
2025-06-16 04:26:24,425 - [INFO] Batch 901 Loss: 0.4341
2025-06-16 04:27:15,302 - [INFO] Batch 1001 Loss: 0.4170
2025-06-16 04:28:06,267 - [INFO] Batch 1101 Loss: 0.4065
2025-06-16 04:28:57,167 - [INFO] Batch 1201 Loss: 0.4165
2025-06-16 04:29:48,096 - [INFO] Batch 1301 Loss: 0.4615
2025-06-16 04:30:38,900 - [INFO] Batch 1401 Loss: 0.4202
2025-06-16 04:31:29,699 - [INFO] Batch 1501 Loss: 0.4414
2025-06-16 04:32:20,679 - [INFO] Batch 1601 Loss: 0.4454
2025-06-16 04:33:11,505 - [INFO] Batch 1701 Loss: 0.4240
2025-06-16 04:35:05,099 - [INFO] ================================================================================
2025-06-16 04:35:05,100 - [INFO] Train Loss: 0.4353 | Train AUPRC: 0.4631 | Val Loss: 0.4229 | Val AUPRC: 0.4977 | LR: 1.00e-03
2025-06-16 04:35:05,100 - [INFO] ================================================================================
2025-06-16 04:35:06,920 - [INFO] Checkpoint saved to ./models/distilbert_cls_frozen_bert/checkpoint.pth
2025-06-16 04:35:06,920 - [INFO] 

2025-06-16 04:35:06,920 - [INFO] Epoch 7 / 10
2025-06-16 04:35:06,920 - [INFO] ------------------------------
2025-06-16 04:35:07,914 - [INFO] Batch 1 Loss: 0.3812
2025-06-16 04:35:58,760 - [INFO] Batch 101 Loss: 0.4421
2025-06-16 04:36:49,581 - [INFO] Batch 201 Loss: 0.3810
2025-06-16 04:37:40,413 - [INFO] Batch 301 Loss: 0.4846
2025-06-16 04:38:31,244 - [INFO] Batch 401 Loss: 0.4073
2025-06-16 04:39:22,147 - [INFO] Batch 501 Loss: 0.4582
2025-06-16 04:40:13,088 - [INFO] Batch 601 Loss: 0.4223
2025-06-16 04:41:03,995 - [INFO] Batch 701 Loss: 0.3521
2025-06-16 04:41:54,896 - [INFO] Batch 801 Loss: 0.4036
2025-06-16 04:42:45,780 - [INFO] Batch 901 Loss: 0.4140
2025-06-16 04:43:36,709 - [INFO] Batch 1001 Loss: 0.4198
2025-06-16 04:44:27,627 - [INFO] Batch 1101 Loss: 0.4393
2025-06-16 04:45:18,516 - [INFO] Batch 1201 Loss: 0.4246
2025-06-16 04:46:09,354 - [INFO] Batch 1301 Loss: 0.4429
2025-06-16 04:47:00,272 - [INFO] Batch 1401 Loss: 0.4199
2025-06-16 04:47:51,092 - [INFO] Batch 1501 Loss: 0.4517
2025-06-16 04:48:41,993 - [INFO] Batch 1601 Loss: 0.4124
2025-06-16 04:49:32,895 - [INFO] Batch 1701 Loss: 0.4816
2025-06-16 04:51:26,433 - [INFO] ================================================================================
2025-06-16 04:51:26,433 - [INFO] Train Loss: 0.4332 | Train AUPRC: 0.4665 | Val Loss: 0.4147 | Val AUPRC: 0.5039 | LR: 1.00e-03
2025-06-16 04:51:26,434 - [INFO] ================================================================================
2025-06-16 04:51:28,175 - [INFO] Checkpoint saved to ./models/distilbert_cls_frozen_bert/checkpoint.pth
2025-06-16 04:51:28,175 - [INFO] 

2025-06-16 04:51:28,175 - [INFO] Epoch 8 / 10
2025-06-16 04:51:28,175 - [INFO] ------------------------------
2025-06-16 04:51:29,185 - [INFO] Batch 1 Loss: 0.4628
2025-06-17 02:06:56,233 - [INFO] Tokenizer loaded: distilbert-base-uncased
2025-06-17 02:16:27,055 - [INFO] Dataset loaded and tokenized.
2025-06-17 02:16:27,056 - [INFO] Data loaders created.
2025-06-17 02:16:30,533 - [INFO] Model initialized.
2025-06-17 02:16:30,534 - [INFO] Optimizer initialized: AdamW with lr = 0.001
2025-06-17 02:16:30,535 - [INFO] Scheduler initialized. Mode = min, factor = 0.5 and patience = 2
2025-06-17 02:16:31,136 - [INFO] Loss function initialized.
2025-06-17 02:16:32,548 - [INFO] Loaded checkpoint from ./models/distilbert_cls_frozen_bert/checkpoint.pth
2025-06-17 02:16:32,549 - [INFO] Resuming training from epoch 8 with best_val_auprc = 0.5039
2025-06-17 02:16:32,550 - [INFO] Epoch 8 / 10
2025-06-17 02:16:32,550 - [INFO] ------------------------------
2025-06-17 02:16:34,153 - [INFO] Batch 1 Loss: 0.4040
2025-06-17 02:17:24,243 - [INFO] Batch 101 Loss: 0.4809
2025-06-17 02:18:14,572 - [INFO] Batch 201 Loss: 0.3818
2025-06-17 02:19:04,923 - [INFO] Batch 301 Loss: 0.3875
2025-06-17 02:19:55,258 - [INFO] Batch 401 Loss: 0.3979
2025-06-17 02:20:45,572 - [INFO] Batch 501 Loss: 0.3820
2025-06-17 02:21:35,936 - [INFO] Batch 601 Loss: 0.4532
2025-06-17 02:22:26,314 - [INFO] Batch 701 Loss: 0.3749
2025-06-17 02:23:16,582 - [INFO] Batch 801 Loss: 0.4249
2025-06-17 02:24:06,954 - [INFO] Batch 901 Loss: 0.3862
2025-06-17 02:24:57,386 - [INFO] Batch 1001 Loss: 0.4369
2025-06-17 02:25:47,744 - [INFO] Batch 1101 Loss: 0.4591
2025-06-17 02:26:38,070 - [INFO] Batch 1201 Loss: 0.4213
2025-06-17 02:27:28,395 - [INFO] Batch 1301 Loss: 0.4356
2025-06-17 02:28:18,815 - [INFO] Batch 1401 Loss: 0.4530
2025-06-17 02:29:09,200 - [INFO] Batch 1501 Loss: 0.4419
2025-06-17 02:29:59,504 - [INFO] Batch 1601 Loss: 0.4060
2025-06-17 02:30:49,863 - [INFO] Batch 1701 Loss: 0.4251
2025-06-17 02:32:41,688 - [INFO] ================================================================================
2025-06-17 02:32:41,688 - [INFO] Train Loss: 0.4309 | Train AUPRC: 0.4686 | Val Loss: 0.4155 | Val AUPRC: 0.5096 | LR: 1.00e-03
2025-06-17 02:32:41,688 - [INFO] ================================================================================
2025-06-17 02:32:43,435 - [INFO] Checkpoint saved to ./models/distilbert_cls_frozen_bert/checkpoint.pth
2025-06-17 02:32:43,435 - [INFO] 

2025-06-17 02:32:43,435 - [INFO] Epoch 9 / 10
2025-06-17 02:32:43,436 - [INFO] ------------------------------
2025-06-17 02:32:44,390 - [INFO] Batch 1 Loss: 0.3630
2025-06-17 02:33:34,772 - [INFO] Batch 101 Loss: 0.4900
2025-06-17 02:34:25,071 - [INFO] Batch 201 Loss: 0.3553
2025-06-17 02:35:15,397 - [INFO] Batch 301 Loss: 0.4277
2025-06-17 02:36:05,721 - [INFO] Batch 401 Loss: 0.3863
2025-06-17 02:36:56,091 - [INFO] Batch 501 Loss: 0.4045
2025-06-17 02:37:46,408 - [INFO] Batch 601 Loss: 0.4515
2025-06-17 02:38:36,752 - [INFO] Batch 701 Loss: 0.4568
2025-06-17 02:39:27,117 - [INFO] Batch 801 Loss: 0.4752
2025-06-17 02:40:17,472 - [INFO] Batch 901 Loss: 0.4758
2025-06-17 02:41:07,789 - [INFO] Batch 1001 Loss: 0.4269
2025-06-17 02:41:58,140 - [INFO] Batch 1101 Loss: 0.4021
2025-06-17 02:42:48,491 - [INFO] Batch 1201 Loss: 0.4041
2025-06-17 02:43:38,863 - [INFO] Batch 1301 Loss: 0.3718
2025-06-17 02:44:29,225 - [INFO] Batch 1401 Loss: 0.4390
2025-06-17 02:45:19,541 - [INFO] Batch 1501 Loss: 0.5426
2025-06-17 02:46:09,896 - [INFO] Batch 1601 Loss: 0.4450
2025-06-17 02:47:00,245 - [INFO] Batch 1701 Loss: 0.4165
2025-06-17 02:48:52,008 - [INFO] ================================================================================
2025-06-17 02:48:52,009 - [INFO] Train Loss: 0.4297 | Train AUPRC: 0.4706 | Val Loss: 0.4109 | Val AUPRC: 0.5091 | LR: 1.00e-03
2025-06-17 02:48:52,009 - [INFO] ================================================================================
2025-06-17 02:48:52,009 - [INFO] No improvement in validation AUPRC for 1 epochs.
2025-06-17 02:48:52,009 - [INFO] 

2025-06-17 02:48:52,009 - [INFO] Epoch 10 / 10
2025-06-17 02:48:52,009 - [INFO] ------------------------------
2025-06-17 02:48:52,983 - [INFO] Batch 1 Loss: 0.4163
2025-06-17 02:49:43,389 - [INFO] Batch 101 Loss: 0.4774
2025-06-17 02:50:33,781 - [INFO] Batch 201 Loss: 0.4248
2025-06-17 02:51:24,163 - [INFO] Batch 301 Loss: 0.4498
2025-06-17 02:52:14,472 - [INFO] Batch 401 Loss: 0.3802
2025-06-17 02:53:04,818 - [INFO] Batch 501 Loss: 0.4567
2025-06-17 02:53:55,219 - [INFO] Batch 601 Loss: 0.4009
2025-06-17 02:54:45,585 - [INFO] Batch 701 Loss: 0.4233
2025-06-17 02:55:35,985 - [INFO] Batch 801 Loss: 0.4920
2025-06-17 02:56:26,312 - [INFO] Batch 901 Loss: 0.4367
2025-06-17 02:57:16,686 - [INFO] Batch 1001 Loss: 0.4651
2025-06-17 02:58:06,983 - [INFO] Batch 1101 Loss: 0.4366
2025-06-17 02:58:57,293 - [INFO] Batch 1201 Loss: 0.4293
2025-06-17 02:59:47,554 - [INFO] Batch 1301 Loss: 0.3940
2025-06-17 03:00:37,909 - [INFO] Batch 1401 Loss: 0.4617
2025-06-17 03:01:28,266 - [INFO] Batch 1501 Loss: 0.4132
2025-06-17 03:02:18,608 - [INFO] Batch 1601 Loss: 0.4562
2025-06-17 03:03:09,000 - [INFO] Batch 1701 Loss: 0.4373
2025-06-17 03:05:00,864 - [INFO] ================================================================================
2025-06-17 03:05:00,864 - [INFO] Train Loss: 0.4290 | Train AUPRC: 0.4712 | Val Loss: 0.4133 | Val AUPRC: 0.5061 | LR: 1.00e-03
2025-06-17 03:05:00,864 - [INFO] ================================================================================
2025-06-17 03:05:00,864 - [INFO] No improvement in validation AUPRC for 2 epochs.
2025-06-17 03:05:00,865 - [INFO] 

2025-06-17 03:05:00,897 - [INFO] Training complete!
2025-06-17 03:05:01,283 - [INFO] Model and training history saved successfully.
2025-06-17 03:05:01,588 - [INFO] Loss plot saved!
2025-06-17 03:05:01,588 - [INFO] Evaluating on threshold tuning set...
2025-06-17 03:06:21,059 - [INFO] Finding the best threshold...
2025-06-17 03:06:23,592 - [INFO] Best threshold: 0.8200 | Best score: 0.4959
2025-06-17 03:06:23,592 - [INFO] Evaluating on test set...
2025-06-17 03:09:02,148 - [INFO] Saving test predictions and evaluation plots...
2025-06-17 03:09:03,359 - [INFO] Test predictions and evaluation plots saved!
2025-06-17 03:09:03,359 - [INFO] Training and evaluation complete!
