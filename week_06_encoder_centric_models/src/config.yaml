seed: 128

model: 
  model_name: 'distilbert-base-uncased'
  num_classes: 2 
  classifier_dim: 128
  dropout: 0.1
  use_cls: True 
  freeze_bert: True

dataset: 
  max_length: 200
  binary_col: 'toxicity'
  thresh_val_size: 0.5
  random_state: 42

dataloader: 
  num_workers: 4
  pin_memory: True
  prefetch_factor: 2

training: 
  batch_size: 128
  class_weights: 'equal'
  epochs: 1
  use_class_weights: True 
  class_weights_strategy: 'balanced' # can be 'balanced', 'none', or a list/tuple of weights (ex: (0.1, 0.9))
  print_every: 2
  patience: 3
  resume: False

optimizer: 
  name: 'AdamW', 
  lr: 0.001

scheduler: 
  use_scheduler: True
  patience: 2 
  factor: 0.5
  mode: 'min'

logging: 
  filemode: 'a'

paths:
  model_dir: './models/distilbert_cls_frozen_bert'
  model_file: 'model.pth'
  checkpoint_file: 'checkpoint.pth'
  train_loss_file: 'train_losses.pt'
  val_loss_file: 'val_losses.pt'
  train_log: 'train.log'
