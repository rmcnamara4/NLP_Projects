seed: 42
resume: true
paths:
  checkpoint_dir: checkpoints/${project.name}_${project.version}
  log_dir: outputs/${project.name}_${project.version}/lightning_logs
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0001
  weight_decay: 0.001
_callback_dict:
  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: val_loss
    patience: 3
    mode: min
    verbose: true
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: val_loss
    mode: min
    save_top_k: 1
    save_weights_only: false
    dirpath: ${paths.checkpoint_dir}
    filename: best_checkpoint
    save_on_train_epoch_end: false
scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: ${trainer.max_epochs}
  eta_min: 0.0
datamodule:
  train_batch_size: 4
  test_batch_size: 16
  chunk_len: 512
  stride: 412
  min_len: 64
  max_len: 1024
  num_workers: 8
  prefetch_factor: 2
  split_sizes:
  - 13000
  - 2000
  - 2000
  chunking_strategy: dynamic
  num_keep: 6
  embedding_model_name: all-MiniLM-L6-v2
  seed: 156
save_model:
  save_model: true
  save_path: models/
  model_name: ${project.name}_${project.version}
  save_format: state_dict
project:
  name: pegasus_dynamic_chunking
  version: v1
model:
  model_name: google/pegasus-pubmed
  dropout: 0.1
  attention_dropout: 0.1
  gradual_unfreeze: false
  max_unfrozen_layers: 0
trainer:
  max_epochs: 10
  accelerator: cuda
  devices: 1
  precision: 16-mixed
  gradient_clip_val: 1.0
  accumulate_grad_batches: 8
  val_check_interval: 0.5
  deterministic: true
  log_every_n_steps: 10
  num_sanity_val_steps: 0
_generation_dict:
  chunk_generation:
    num_beams: 5
    do_sample: false
    early_stopping: true
    max_new_tokens: 120
    repetition_penalty: 1.2
    length_penalty: 1.2
    no_repeat_ngram_size: 3
  final_generation:
    num_beams: 5
    do_sample: false
    early_stopping: true
    max_new_tokens: 300
    repetition_penalty: 1.2
    length_penalty: 1.2
    no_repeat_ngram_size: 3
lora:
  use_lora: true
  rank: 8
  alpha: 16
  target_modules:
  - q_proj
  - v_proj
  bias: none
  dropout: 0.1
