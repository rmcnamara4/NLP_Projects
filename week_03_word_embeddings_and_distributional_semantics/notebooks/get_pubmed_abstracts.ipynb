{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download PubMed Abstracts"
      ],
      "metadata": {
        "id": "y386HJiK5RBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note**: This notebook is compatible with both Google Colab and local Jupyter environments. Colab-specific sections are clearly marked."
      ],
      "metadata": {
        "id": "CDURZr4SVBaw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, I collect a large set of PubMed abstracts using the Entrez API to build a domain-specific corpus for word embedding training. I use a MeSH-based search to retrieve relevant articles, filter for those with available abstracts, and save the cleaned data to CSV for downstream NLP tasks. This corpus will serve as the foundation for comparing embedding quality across different tokenization methods in a biomedical context."
      ],
      "metadata": {
        "id": "Pz9qAdvJUdBz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQd6hZhH5MOi",
        "outputId": "f6a6668d-5068-4fa8-d595-d9007140212a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: biopython in /usr/local/lib/python3.11/dist-packages (1.85)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "# install biopython\n",
        "!pip install biopython"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "TUq_u9iL5aNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    project_path = '/content/drive/MyDrive/NLP_Projects/Week_3/word-embeddings-playground'\n",
        "    if os.path.exists(project_path):\n",
        "        os.chdir(project_path)\n",
        "        print(f\"Changed working directory to: {project_path}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Project path not found: {project_path}\")\n",
        "else:\n",
        "    print(\"Not running in Colab — skipping Drive mount.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVFMSzEa9omN",
        "outputId": "8f935d7b-2ca8-4828-8bb9-cb7b559648cc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Changed working directory to: /content/drive/MyDrive/NLP_Projects/Week_3/word-embeddings-playground\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "from Bio import Entrez, Medline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time"
      ],
      "metadata": {
        "id": "3GACJVzD5Y7r"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collecting PubMed Abstracts Using Entrez and Medline\n",
        "\n",
        "In this section, we collect biomedical abstracts from PubMed using the NCBI Entrez API.\n",
        "\n",
        "1. **Set up Entrez access**: We begin by specifying a contact email (required by NCBI) to identify ourselves in API requests.\n",
        "2. **Search for articles**: We define a query using the MeSH term `\"medicine\"` and retrieve up to 100,000 PubMed IDs that match the search.\n",
        "3. **Fetch abstracts**: We then download the abstracts in batches of 500 using the `efetch` endpoint. For each article, we extract:\n",
        "   - PMID (PubMed ID)\n",
        "   - Title\n",
        "   - Abstract text\n",
        "\n",
        "We add a 1-second delay between batches to respect NCBI’s rate limits."
      ],
      "metadata": {
        "id": "yn24HhIZT_T_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Entrez.email = 'rymcnamara4@gmail.com'\n",
        "\n",
        "search_query = 'medicine[MeSH Terms]'"
      ],
      "metadata": {
        "id": "ymPvBJiv5vEa"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_abstracts(query, max_results = 100_000):\n",
        "  \"\"\"\n",
        "  Search PubMed for article IDs using a specified query.\n",
        "\n",
        "  Parameters:\n",
        "      query (str): The search term or MeSH query to run against PubMed.\n",
        "      max_results (int, optional): Maximum number of PubMed IDs to retrieve. Default is 100,000.\n",
        "\n",
        "  Returns:\n",
        "      list: A list of PubMed IDs (PMIDs) matching the query.\n",
        "  \"\"\"\n",
        "  handle = Entrez.esearch(db = 'pubmed', term = query, retmax = max_results, retmode = 'xml')\n",
        "  record = Entrez.read(handle)\n",
        "  return record['IdList']"
      ],
      "metadata": {
        "id": "ZRvroMkL6CQr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = search_abstracts(search_query)\n",
        "print(f'Found {len(ids)} abstracts.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9T0qv9hp6eR4",
        "outputId": "1bd62ab1-d0af-49c0-f6f7-7e0dee89d936"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9999 abstracts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_abstracts(ids, batch_size = 500):\n",
        "  \"\"\"\n",
        "  Fetch PubMed article metadata (title, abstract, PMID) for a list of IDs.\n",
        "\n",
        "  Parameters:\n",
        "      ids (list): A list of PubMed IDs (PMIDs) to retrieve data for.\n",
        "      batch_size (int, optional): Number of articles to fetch per API call. Default is 500.\n",
        "\n",
        "  Returns:\n",
        "      list: A list of dictionaries, each containing 'PMID', 'Title', and 'Abstract' for one article.\n",
        "  \"\"\"\n",
        "  abstracts = []\n",
        "  for i in range(0, len(ids), batch_size):\n",
        "    batch_ids = ids[i:i + batch_size]\n",
        "    fetch_handle = Entrez.efetch(db = 'pubmed', id = ','.join(batch_ids), rettype = 'medline', retmode = 'text')\n",
        "    records = Medline.parse(fetch_handle)\n",
        "    for rec in records:\n",
        "      title = rec.get('TI', 'No Title')\n",
        "      abstract = rec.get('AB', 'No Abstract')\n",
        "      id = rec.get('PMID', 'Unknown')\n",
        "      abstracts.append({'PMID': id, 'Title': 'title', 'Abstract': abstract})\n",
        "\n",
        "    print(f'Fetched {i + len(batch_ids)} abstracts so far...')\n",
        "    time.sleep(1)\n",
        "\n",
        "  return abstracts"
      ],
      "metadata": {
        "id": "ZzrTjtBC7niV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abstracts = fetch_abstracts(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCD7gop58Qim",
        "outputId": "11c10d83-6f2e-4564-c6e3-2657c8f07f60"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched 500 abstracts so far...\n",
            "Fetched 1000 abstracts so far...\n",
            "Fetched 1500 abstracts so far...\n",
            "Fetched 2000 abstracts so far...\n",
            "Fetched 2500 abstracts so far...\n",
            "Fetched 3000 abstracts so far...\n",
            "Fetched 3500 abstracts so far...\n",
            "Fetched 4000 abstracts so far...\n",
            "Fetched 4500 abstracts so far...\n",
            "Fetched 5000 abstracts so far...\n",
            "Fetched 5500 abstracts so far...\n",
            "Fetched 6000 abstracts so far...\n",
            "Fetched 6500 abstracts so far...\n",
            "Fetched 7000 abstracts so far...\n",
            "Fetched 7500 abstracts so far...\n",
            "Fetched 8000 abstracts so far...\n",
            "Fetched 8500 abstracts so far...\n",
            "Fetched 9000 abstracts so far...\n",
            "Fetched 9500 abstracts so far...\n",
            "Fetched 9999 abstracts so far...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert and Save Abstracts to CSV\n",
        "\n",
        "After collecting the PubMed abstracts, we convert the data into a pandas DataFrame for easier processing and analysis.\n",
        "\n",
        "Steps:\n",
        "1. Convert the list of abstracts to a `DataFrame`.\n",
        "2. Filter out entries that contain no abstract text.\n",
        "3. Print the number of remaining abstracts.\n",
        "4. Save the cleaned DataFrame to a CSV file in the `data/` directory."
      ],
      "metadata": {
        "id": "khiS61cG8wC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "abstracts_df = pd.DataFrame(abstracts)"
      ],
      "metadata": {
        "id": "onCB9XTr8ZL8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abstracts_df = abstracts_df[abstracts_df['Abstract'] != 'No Abstract']"
      ],
      "metadata": {
        "id": "TVYZ57mu9FZZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'There are {len(abstracts_df)} abstracsts.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z23w8Xuw9GB5",
        "outputId": "8f0c520e-47a7-4fef-eafc-591edbc08747"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 9982 abstracsts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abstracts_df.to_csv('./data/pubmed_abstracts.csv', header = True, index = False)"
      ],
      "metadata": {
        "id": "9baIUVXU9bff"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Cqfk7eyQX4N"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}