{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Similarity Evaluation"
      ],
      "metadata": {
        "id": "fhXN5n1LR1S1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note**: This notebook is compatible with both Google Colab and local Jupyter environments. Colab-specific sections are clearly marked."
      ],
      "metadata": {
        "id": "WQQjKUvjXQxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook evaluates and compares two embedding models trained on biomedical text:\n",
        "\n",
        "- **Word-level Skip-Gram** (trained on whole, lemmatized words)\n",
        "- **Subword-level Skip-Gram** (trained on Byte Pair Encoded tokens)\n",
        "\n",
        "Using the UMNSRS similarity dataset, we assess how well each model captures semantic similarity between biomedical terms. We evaluate on both the full dataset and a filtered subset to ensure a fair comparison."
      ],
      "metadata": {
        "id": "MezcqDuBXe9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U nltk datasets gensim swifter tokenizers --no-cache-dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BXdX5p4R51_",
        "outputId": "634b5acf-30c1-49f1-c1c2-9927868f05d9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: swifter in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: psutil>=5.6.6 in /usr/local/lib/python3.11/dist-packages (from swifter) (5.9.5)\n",
            "Requirement already satisfied: dask>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]>=2.10.0->swifter) (2024.12.1)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.1.1)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.4.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (0.12.1)\n",
            "Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.7.0)\n",
            "Requirement already satisfied: dask-expr<1.2,>=1.1 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]>=2.10.0->swifter) (1.1.21)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.13.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.21.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_ypc7klQHAu",
        "outputId": "e91c3151-f1c5-4847-a074-357a5ef060dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Changed working directory to: /content/drive/MyDrive/NLP_Projects/Week_3/word-embeddings-playground\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    project_path = '/content/drive/MyDrive/NLP_Projects/Week_3/word-embeddings-playground'\n",
        "    if os.path.exists(project_path):\n",
        "        os.chdir(project_path)\n",
        "        print(f\"Changed working directory to: {project_path}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Project path not found: {project_path}\")\n",
        "else:\n",
        "    print(\"Not running in Colab — skipping Drive mount.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "A_wnkJWeSLl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "from scipy.stats import spearmanr, pearsonr"
      ],
      "metadata": {
        "id": "J67G_VitSITC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Embeddings, Tokenizer, and Evaluation Dataset\n",
        "\n",
        "This section loads the pretrained word embedding models, the BPE tokenizer, and the UMNSRS biomedical similarity dataset for evaluation.\n",
        "\n",
        "- **Embeddings**:\n",
        "  - `skip_gram_embeddings`: Word-level Skip-Gram vectors\n",
        "  - `bpe_embeddings`: Subword-level Skip-Gram vectors trained on BPE tokens\n",
        "\n",
        "- **Tokenizer**:\n",
        "  - Trained BPE tokenizer (used for segmenting terms into subwords)\n",
        "\n",
        "- **Evaluation Dataset**:\n",
        "  - [UMNSRS (Unified Medical Language System - Similarity and Relatedness Set)](https://huggingface.co/datasets/bigbio/umnsrs)\n",
        "  - Contains pairs of biomedical terms with human-annotated similarity scores"
      ],
      "metadata": {
        "id": "b-KH6yCgSLRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_embeddings = KeyedVectors.load('./results/bpe_skipgram/bpe_embeddings.embeddings')\n",
        "skip_gram_embeddings = KeyedVectors.load('./results/skipgram/skipgram.embeddings')\n",
        "\n",
        "bpe_tokenizer = Tokenizer.from_file('./data/bpe_skipgram/bpe_tokenizer.json')\n",
        "\n",
        "if os.path.exists('./data/umnsrs_data.csv'):\n",
        "    data = pd.read_csv('./data/umnsrs_data.csv')\n",
        "else:\n",
        "    data = load_dataset('bigbio/umnsrs')\n",
        "    data = pd.DataFrame(data['train'])\n",
        "    data = data[['text_1', 'text_2', 'mean_score']]\n",
        "\n",
        "    data.to_csv('./data/umnsrs_data.csv', header = True, index = False)"
      ],
      "metadata": {
        "id": "Al-1H6J-SpHJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Functions\n",
        "\n",
        "This section defines utility functions used to compute model-based similarity scores and evaluate embedding quality against the UMNSRS biomedical dataset.\n",
        "\n",
        "- **`get_bpe_embedding()`**  \n",
        "  Computes the embedding for a word using BPE subword tokens. If the full word is not in the vocabulary, it averages the embeddings of its BPE tokens.\n",
        "\n",
        "- **`get_similarity()`**  \n",
        "  Calculates the cosine similarity between two terms using either word-level or BPE-based embeddings, depending on the `bpe` flag.\n",
        "\n",
        "- **`evaluate()`**  \n",
        "  Iterates over all term pairs in the dataset, computes similarity scores using the chosen embedding model, and compares them with human-annotated similarity scores using Spearman and Pearson correlation.\n",
        "\n",
        "These functions form the backbone of the quantitative evaluation used to compare the two embedding approaches."
      ],
      "metadata": {
        "id": "QyACi_qITcXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bpe_embedding(word, tokenizer, embeddings):\n",
        "  \"\"\"\n",
        "  Retrieves the embedding for a given word using BPE subword embeddings.\n",
        "\n",
        "  If the word exists in the embedding vocabulary, its embedding is returned directly.\n",
        "  Otherwise, the word is tokenized into subwords using the BPE tokenizer, and the\n",
        "  average of the available subword embeddings is returned.\n",
        "\n",
        "  Args:\n",
        "      word (str): The word to retrieve an embedding for.\n",
        "      tokenizer (tokenizers.Tokenizer): A pretrained Hugging Face BPE tokenizer.\n",
        "      embeddings (gensim KeyedVectors): Trained BPE word vector model.\n",
        "\n",
        "  Returns:\n",
        "      np.ndarray or None: The embedding vector for the word, or None if no tokens are found in the vocabulary.\n",
        "  \"\"\"\n",
        "  if word in embeddings:\n",
        "    return embeddings[word]\n",
        "  else:\n",
        "    tokens = tokenizer.encode(word).tokens\n",
        "    vectors = [embeddings[token] for token in tokens if token in embeddings]\n",
        "    if vectors:\n",
        "      return np.mean(vectors, axis = 0)\n",
        "    else:\n",
        "      return None"
      ],
      "metadata": {
        "id": "i61nRrKkTRUA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similarity(w1, w2, embeddings, bpe = False):\n",
        "  \"\"\"\n",
        "  Computes the cosine similarity between two biomedical terms using either word-level\n",
        "  or subword-level (BPE) embeddings.\n",
        "\n",
        "  - If `bpe=True`, each word is converted to its BPE-based embedding using the average\n",
        "    of its subword vectors (via `get_bpe_embedding`).\n",
        "  - If `bpe=False`, the words must exist in the embedding vocabulary directly.\n",
        "\n",
        "  Args:\n",
        "      w1 (str): First word or term.\n",
        "      w2 (str): Second word or term.\n",
        "      embeddings (gensim KeyedVectors): Word or BPE embedding model.\n",
        "      bpe (bool, optional): Whether to use BPE embeddings. Defaults to False.\n",
        "\n",
        "  Returns:\n",
        "      float or None: Cosine similarity between the two terms, or None if one or both embeddings are unavailable.\n",
        "  \"\"\"\n",
        "  if bpe:\n",
        "    w1 = get_bpe_embedding(w1, bpe_tokenizer, embeddings)\n",
        "    w2 = get_bpe_embedding(w2, bpe_tokenizer, embeddings)\n",
        "  else:\n",
        "    if w1 in embeddings and w2 in embeddings:\n",
        "      w1 = embeddings[w1]\n",
        "      w2 = embeddings[w2]\n",
        "    else:\n",
        "      return None\n",
        "  return cosine_similarity([w1], [w2])[0][0]"
      ],
      "metadata": {
        "id": "lVtLwTR6UMQ6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(data, embeddings, bpe = False):\n",
        "  \"\"\"\n",
        "  Evaluates a word embedding model by computing similarity correlations against\n",
        "  human-annotated biomedical term pairs.\n",
        "\n",
        "  - For each term pair in the dataset, the model-based cosine similarity is computed.\n",
        "  - If `bpe=True`, subword-level embeddings are used via `get_bpe_embedding`.\n",
        "  - Only pairs where both terms have valid embeddings are included.\n",
        "\n",
        "  Evaluation metrics:\n",
        "  - **Spearman correlation**: Measures rank agreement with human scores\n",
        "  - **Pearson correlation**: Measures linear correlation with human scores\n",
        "  - **Number Evaluated**: Number of pairs included in the evaluation\n",
        "\n",
        "  Args:\n",
        "      data (pd.DataFrame): Evaluation dataset with columns `text_1`, `text_2`, and `mean_score`.\n",
        "      embeddings (gensim KeyedVectors): Word or subword embedding model.\n",
        "      bpe (bool, optional): Whether to use BPE-based subword embeddings. Defaults to False.\n",
        "\n",
        "  Returns:\n",
        "      inds (List[int]): Indices of rows that were successfully evaluated.\n",
        "      pd.Series: Spearman correlation, Pearson correlation, and number of evaluated pairs.\n",
        "  \"\"\"\n",
        "  similarities = []\n",
        "  human_scores = []\n",
        "  inds = []\n",
        "  for i, row in data.iterrows():\n",
        "    sim = get_similarity(row['text_1'].lower(), row['text_2'].lower(), embeddings, bpe)\n",
        "    if sim is not None:\n",
        "      inds.append(i)\n",
        "      similarities.append(sim)\n",
        "      human_scores.append(row['mean_score'])\n",
        "\n",
        "  spearman_score = spearmanr(similarities, human_scores)[0]\n",
        "  pearson_score = pearsonr(similarities, human_scores)[0]\n",
        "  number_evaluated = len(similarities)\n",
        "\n",
        "  print('Spearman:', spearman_score)\n",
        "  print('Pearson:', pearson_score)\n",
        "  print('Number Evaluated:', number_evaluated)\n",
        "  return inds, pd.Series([spearman_score, pearson_score, number_evaluated])"
      ],
      "metadata": {
        "id": "35y33WphVlZ7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantitative Evaluation on UMNSRS Dataset\n",
        "\n",
        "We evaluate both the word-level Skip-Gram model and the subword-level BPE model on the UMNSRS biomedical similarity dataset.\n",
        "\n",
        "- **Skip-Gram**: Only evaluates term pairs where both full words exist in the vocabulary.\n",
        "- **BPE**: Can evaluate far more pairs by averaging subword embeddings when full words are missing."
      ],
      "metadata": {
        "id": "6n3nEz2XXMLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full Dataset Evaluation\n",
        "\n",
        "- **Skip-Gram**\n",
        "  - Spearman: **0.361**\n",
        "  - Pearson: **0.452**\n",
        "  - Term Pairs Evaluated: 33\n",
        "\n",
        "- **BPE**\n",
        "  - Spearman: **0.090**\n",
        "  - Pearson: **0.153**\n",
        "  - Term Pairs Evaluated: 566\n",
        "\n",
        "While BPE covers more terms, its overall alignment with human judgments is weaker across the full dataset."
      ],
      "metadata": {
        "id": "KYT42hH3W7mN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "skip_gram_inds, skip_gram_results = evaluate(data, skip_gram_embeddings, bpe = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pRLbY1AWQTH",
        "outputId": "dd280342-3c60-4cd1-a0b1-0460917af7ea"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spearman: 0.36115985753236607\n",
            "Pearson: 0.4515940312593645\n",
            "Number Evaluated: 33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_inds, bpe_results = evaluate(data, bpe_embeddings, bpe = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Cf0jFmBVpf9",
        "outputId": "459b702b-e12b-4f82-f829-a703e0712023"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spearman: 0.0899903339261131\n",
            "Pearson: 0.15292074875644843\n",
            "Number Evaluated: 566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⚖️ Filtered Evaluation (Skip-Gram-Compatible Pairs Only)\n",
        "\n",
        "To isolate model quality from vocabulary coverage, we compare both models on the **exact 27 term pairs** that the Skip-Gram model was able to process.\n",
        "\n",
        "- **Skip-Gram**\n",
        "  - Spearman: **0.361**\n",
        "  - Pearson: **0.452**\n",
        "\n",
        "- **BPE**\n",
        "  - Spearman: **0.333**\n",
        "  - Pearson: **0.451**\n",
        "\n",
        "On this shared subset, BPE performs comparably to Skip-Gram, suggesting strong potential when vocabulary coverage is not a limiting factor."
      ],
      "metadata": {
        "id": "qQQKiDgBYYnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_data = data.iloc[skip_gram_inds, :]"
      ],
      "metadata": {
        "id": "CLz4x7iwWG_h"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, skip_gram_results_filtered = evaluate(filtered_data, skip_gram_embeddings, bpe = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4C2xiZjYuIz",
        "outputId": "66dae8bb-3b49-4ef5-ee30-87df39419fd0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spearman: 0.36115985753236607\n",
            "Pearson: 0.4515940312593645\n",
            "Number Evaluated: 33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, bpe_results_filtered = evaluate(filtered_data, bpe_embeddings, bpe = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gMYFSjTY0wH",
        "outputId": "d2b0a8f7-279d-47a0-e97b-9201b5e339e2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spearman: 0.3327483925714673\n",
            "Pearson: 0.4509520721906824\n",
            "Number Evaluated: 33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame([\n",
        "    ['Skip-Gram (Full Eval)', skip_gram_results[0], skip_gram_results[1], skip_gram_results[2]],\n",
        "    ['BPE (Full Eval)', bpe_results[0], bpe_results[1], bpe_results[2]],\n",
        "    ['Skip-Gram (Filtered)', skip_gram_results_filtered[0], skip_gram_results_filtered[1], skip_gram_results_filtered[2]],\n",
        "    ['BPE (Filtered)', bpe_results_filtered[0], bpe_results_filtered[1], bpe_results_filtered[2]],\n",
        "], columns=['Model', 'Spearman', 'Pearson', 'Num Evaluated'])\n",
        "\n",
        "results_df.to_csv('./results/evaluation_summary.csv', index = False, header = True)"
      ],
      "metadata": {
        "id": "2vWzw7QCY4bG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Takeaways\n",
        "\n",
        "- The **Skip-Gram model** achieved higher correlation with human similarity judgments but was limited to a small subset of term pairs due to vocabulary constraints.\n",
        "- The **BPE model** handled many more term pairs, demonstrating better coverage, and performed competitively on the shared subset.\n",
        "- Subword-based embeddings offer a **scalable and flexible alternative** for biomedical text, particularly when dealing with rare or morphologically complex terms."
      ],
      "metadata": {
        "id": "OhtwC_5vXh0G"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LDh4sV5wXiWp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}