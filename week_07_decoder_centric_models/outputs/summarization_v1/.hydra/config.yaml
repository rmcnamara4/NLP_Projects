seed: 42
resume: true
paths:
  checkpoint_dir: checkpoints/${project.name}_${project.version}
  log_dir: outputs/${project.name}_${project.version}/lightning_logs
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0001
  weight_decay: 0.01
_callback_dict:
  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: val_loss
    patience: 3
    mode: min
    verbose: true
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: val_loss
    mode: min
    save_top_k: 1
    save_weights_only: false
    dirpath: ${paths.checkpoint_dir}
    filename: best_checkpoint
    save_on_train_epoch_end: false
scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: ${trainer.max_epochs}
  eta_min: 0.0
datamodule:
  train_batch_size: 4
  test_batch_size: 8
  chunk_len: 512
  stride: 412
  min_len: 256
  max_len: 1024
  num_workers: 8
  prefetch_factor: 2
  split_sizes:
  - 13000
  - 2000
  - 2000
  padding_value: -100
  seed: 156
save_model:
  save_model: true
  save_path: models/
  model_name: ${project.name}_${project.version}
  save_format: state_dict
project:
  name: summarization
  version: v1
model:
  model_name: gpt2
  attn_pdrop: 0.3
  resid_pdrop: 0.3
  embd_pdrop: 0.3
  gradual_unfreeze: true
trainer:
  max_epochs: 10
  accelerator: cuda
  devices: 1
  precision: 16-mixed
  gradient_clip_val: 1.0
  accumulate_grad_batches: 8
  val_check_interval: 0.5
  deterministic: true
  log_every_n_steps: 10
  num_sanity_val_steps: 0
generation:
  num_beams: 5
  do_sample: false
  early_stopping: true
  max_new_tokens: 200
  repetition_penalty: 1.2
  length_penalty: 1.2
  no_repeat_ngram_size: 3
